<!DOCTYPE html>
<html>
  <head lang="ko">

    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <title>뉴럴 네트워크와 딥러닝</title>

    <link href="assets/style.css" rel="stylesheet">

    <!-- mathjax -->
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
          tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
        });
    </script>
    
    <!-- jQuery -->
    <script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

    <!-- google code-prettify -->
    <script src="https://cdn.jsdelivr.net/gh/google/code-prettify@master/loader/run_prettify.js"></script>  

  </head>
    
  <body>
    
    <div class="header">
      <h1>뉴럴 네트워크와 딥러닝</h1>
      <h3>Original Edition: <a href="http://neuralnetworksanddeeplearning.com/">Neural Networks and Deep Learning</a> by <a href="http://michaelnielsen.org/">Michael A. Nielsen</a></h3>
    </div>
    
    <div class="topnav">
      <a href="#">tmpLink</a>
      <a href="#">tmpLink</a>
      <a href="#">tmpLink</a>
      <a href="https://github.com/sihyeon-kim/neural-networks-and-deep-learning-korean" style="float:right">
        <img src="images/GitHub-Mark-32px.png" widthe="19" height="19">
        &nbsp;Edit on translator's Github
      </a>
    </div>

    
    
    <div class="row">

      <!--
      <div class="sidebar">
        <a class="active" href="#home">Home</a>
        <a href="#news">News</a>
        <a href="#contact">Contact</a>
        <a href="#about">About</a>
      </div>
      -->

      <div class="sidebar">
        <div class="card">
          <a href="index.html">뉴럴 네트워크와 딥러닝</a>
          <a href="aboutBook.html">&nbsp;&nbsp;&nbsp;&nbsp;이 책에 대하여</a>
          <a href="onExercises.html">&nbsp;&nbsp;&nbsp;&nbsp;연습문제에 대하여</a>
          <a class="active" href="chapter1.html">뉴럴 네트워크를 이용하여 손으로 쓴 숫자 인식하기</a>
          <a href="chapter2.html">역전파 알고리즘 동작 방법</a>
          <a href="chapter3.html">뉴럴 네트워크의 학습 방법 향상하기</a>
          <a href="chapter4.html">모든 함수를 계산할 수 있는 뉴럴 네트워크 증명</a>
          <a href="chapter5.html">딥 뉴럴 네트워크를 학습시키기 어려운 이유</a>
          <a href="chapter6.html">딥러닝</a>
          <a href="appendix.html">부록: 지능을 위한 단순한 알고리즘</a>
          <a href="acknowledgements.html">감사의 말</a>
          <a href="faq.html">질문과 답변</a>
        </div>
      </div>

      <div class="leftcolumn">
        <div class="card">
          <h1>1장</h1>
          <h1>뉴럴 네트워크를 이용하여 손으로 쓴 숫자 인식하기</h1>
          <hr>

          <p>
            인간 시각 시스템은 세계 불가사의 중 하나입니다.
            아래 손으로 쓴 숫자를 한 번 보세요.
            <br /><br />
            <img class="contentimg" src="./images/chapter1/digits.png" alt="digits" width="200px">
            <br /><br />
            대부분의 사람은 아무 노력 없이 504192라는 숫자를 알 수 있습니다.
            너무 쉬워서 장난치는 것처럼 느껴지죠.
            인간 뇌의 각 반구에는 V1이라 불리는 일차 시각 피질이 있습니다.
            일차 시각 피질은 14억 개의 뉴런으로 구성되며 뉴런 사이에는 수십억 개의 연결 통로가 있습니다.
            인간 시각 시스템은 V1뿐만 아니라 V2와 V3, V4, V5와 같은 일련의 피질들로 구성되어 있고 더 복잡한 이미지 처리를 합니다. 
            우리는 수억 년에 걸쳐 진화해 시각적인 세상을 이해하는데 잘 적응한 슈퍼컴퓨터를 머리에 지니고 있는 셈입니다. 
            손으로 쓴 숫자를 인식하는 것은 쉬운 일이 아니지만, 
            인간은 꽤 놀라울 정도로 눈으로 본 것을 잘 이해합니다. 
            그리고 거의 모든 과정은 무의식적으로 일어납니다. 
            그래서 보통 우리는 시각 시스템이 얼마나 어려운 문제를 푸는지 가늠할 수 없습니다. 
          </p>

          <p>
            위에서 본 숫자를 인식하는 컴퓨터 프로그램을 작성하려고 할 때 시각 패턴 인식의 어려움은 명확해집니다. 
            눈으로 보고 이해할 때는 쉬운 문제가 갑자기 어렵게 느껴집니다. 
            단순한 직감을 이용하여 모양을 인식하는 방법을 생각해 봅시다. 
            "숫자 9는 위에 고리를 가지고 있고, 오른쪽에 수직선을 가지고 있습니다." 
            이러한 방식으로 알고리즘을 작성하려면 쉽지 않습니다. 
            위와 같은 규칙들을 정확하게 만들려고 하면 수많은 예외와 특수한 경우들의 늪에 빠집니다. 
            절망적이에요. 
          </p>

          <p>
            뉴럴 네트워크는 다른 방식으로 문제에 접근합니다. 
            학습 데이터라 부르는 수많은 손글씨 숫자를 이용하여 학습할 수 있는 시스템을 개발하는 것이 기본적인 아이디어 입니다. 
            <br /><br />
            <img class="contentimg" src="./images/chapter1/mnist_100_digits.png" alt="digits" width="400px">
            <br /><br />
            다시 말해, 뉴럴 네트워크는 데이터 셋을 활용하여 자동으로 손글씨를 인식하는 규칙을 추론합니다. 
            게다가, 학습 데이터 셋의 수가 많을수록 뉴럴 네트워크는 손글씨를 더 학습할 수 있으며 정확도가 향상됩니다. 
            위에서 100개의 학습 데이터를 보여주었지만 학습데이터의 수를 수천 개 혹은 수백만 개로 늘리면 더 좋은 손글씨 인식기를 만들 수 있습니다.  
          </p>
          
          <p>
            1장에서 컴퓨터 프로그램을 작성하여 손글씨 숫자를 인식하는 뉴럴 네트워크를 구현해볼겁니다. 
            프로그램은 단지 74줄 정도이며 다른 뉴럴 네트워크 라이브러리를 사용하지 않을 겁니다. 
            짧은 프로그램이지만 인간의 개입없이 손글씨 숫자를 96퍼센트가 넘는 정확도로 인식할 수 있습니다. 
            실제 상업적으로 이용되는 뉴럴 네트워크는 성능이 훨씬 뛰어납니다. 
            은행에서 수표를 처리하고, 우체국에서는 주소를 인식하기위해 뉴럴 네트워크를 사용하고 있습니다. 
          </p>

          <p>
            손글씨 인식은 일반적인 뉴럴 네트워크를 공부하는데 훌륭한 예제입니다. 
            손글씨 숫자를 인식하는 것은 결코 쉽지 않은 문제이지만 복잡한 해결책이 필요하거나 엄청낭 계산 능력을 요구하는 것만큼 어렵지도 않습니다. 
            게다가 딥러닝같은 심화된 기술들을 공부하는데 좋은 예제입니다. 
            그래서 책 전반에서 손글씨 인식 문제를 다룰것입니다. 
            책의 후반부에서 손글씨를 인식하는 아이디어가 컴퓨터 비전, 음성 인식, 자연어 처리와 같은 다른 분야의 문제에 어떻게 적용되는지 살펴보겠습니다. 
          </p>

          <p>
            물론 1장에서 손글씨 숫자를 인식하는 프로그램을 작성하는데에만 초점을 맞추었다면 1장은 훨씬더 짧았을 거에요. 
            하지만 그 과정에서 뉴럴 네트워크의 핵심 아이디어를 다룰겁니다. 
            중요한 두 가지 인공 뉴런인 퍼셉트론과 시그모이드 뉴런, 뉴럴 네트워크를 위한 표준 학습 알고리즘인 확률적 경사 하강법을 알아보겠습니다. 
            전반적으로 핵심 아이디어가 왜 그런 방식으로 동작하는지, 뉴럴 네트워크에 대한 직관력을 갖추는데 초점을 맞추고 있습니다. 
            기본적인 동작 원리만 설명하는 것보다 더 긴 설명이 필요하지만 심도있게 이해할 가치가 있습니다. 
            1장의 끝부분에 도달하면 딥러닝이 무엇인지, 왜 중요하지 이해할 수 있습니다. 
          </p>

          <br />
          <h2>퍼셉트론</h2>
          <hr>
          
          <p>
            뉴럴 네트워크란 무엇일까요? 
            먼저 인공 뉴런의 한 종류인 퍼셉트론에 대해 설명하겠습니다. 
            1950년대와 1960년대에 과학자 프랭크 로젠블랫이 워렌 맥컬로치와 월터 피츠 초기 논문에 영감을 받아 퍼셉트론을 고안했습니다. 
            현재에는 인공 뉴런의 다른 모델으 더 흔하게 사용되고 있습니다. 
            뉴럴 네트워크를 다루는 현대의 논문에서 사용되는 주된 뉴런 모델은 시그모이드 뉴런입니다. 
            곧 시그모이드 뉴런에 대해 알아보겠지만 시그모이드 뉴런이 이해하기 위해서는 퍼셉트론을 먼저 알아볼 필요가 있습니다. 
          </p>

          <p>
            그렇다면 퍼셉트론은 어떻게 동작할까요? 
            퍼셉트론은 여러 개의 이진 입력 $x_1, x_2, \ldots$을 받아 하나의 이진 값을 출력합니다. 
            <br /><br />
            <img class="contentimg" src="./images/chapter1/tikz0.png" alt="digits" width="300px">
            <br /><br /> 
            위의 그림에서 퍼셉트론은 세 개의 입력 $x_1, x_2, x_3$을 받습니다. 
            일반적으로 입력의 개수가 더 많을수도, 더 적을수도 있습니다. 
            로젠블랫은 출력을 계산하는 간단한 규칙을 제안했습니다. 
            로젠블랫은 가중치 $w_1,w_2,\ldots$를 도입했습니다. 
            가중치는 실수이고 출력에 대한 각 입력의 중요성을 나타냅니다. 
            뉴런의 출력은 0 또는 1이며 가중치 합 $\sum_j w_j x_j$이 역치보다 큰지 작은지에 따라 결정됩니다. 
            역치는 실수이고 뉴런의 파라미터입니다. 
            대수적으로 정확하게 나타내면 아래와 같습니다. 
            $$\begin{eqnarray}
            \mbox{output} & = & \left\{ \begin{array}{ll}
                0 & \mbox{if } \sum_j w_j x_j \leq \mbox{ threshold} \\
                1 & \mbox{if } \sum_j w_j x_j > \mbox{ threshold}
                \end{array} \right.
            \tag{1}\end{eqnarray}$$
            이것이 바로 퍼셉트론이 동작하는 방식입니다. 
          </p>

          <p>
            퍼셉트론은 기본적인 수학적 모델입니다. 
            우리는 퍼셉트론이 증거에 가중치를 매겨 결정을 내리는 장치라고 생각할 수 있습니다. 
            예를 들어 보겠습니다. 
            현실적인 예는 아니지만 이해하기 쉬운 예를 먼저 들고 나중에 조금더 현실적인 예를 다루겠습니다. 
            주말이 다가오고, 당신이 살고있는 곳에서 치즈 페스티벌이 열린다고 생각해봅시다. 
            당신은 치즈를 좋아하고 페스티벌에 가야할지 말아야할지 결정을 내려야합니다. 
            당신은 세 가지 요소를 가늠하여 결정을 내립니다. 
            <ol>
                <li>날씨가 좋은가?</li>
                <li>당신의 이성친구가 함께 가길 원하는가?</li>
                <li>대중교통을 이용해 페스티벌에 갈 수 있는가? (자가용이 없다고 생각합시다.)</li>
            </ol>
            세 가지 요소를 이진 변수 $x_1, x_2, x_3$으로 표현할 수 있습니다. 
            예를 들어, 날씨가 좋으면 $x_1 = 1$ 날씨가 좋지 않으면 $x_1 = 0$으로 나타낼 수 있습니다. 
            비슷한 방법으로 이성친구가 가길 원하면 $x_2 = 1$ 그렇지 않다면 $x_2 = 0$으로 나타냅니다. 
            대중교통에 대해서도 같은 방법으로 $x_3$을 표현할 수 있습니다. 
          </p>

          <p>
            이제 이성친구가 관심이 없어도 페스티벌에 가기 힘들어도 당신이 치즈를 너무 좋아하여 페스티벌에 가고싶다고 가정해보겠습니다. 
            하지만 당신은 날씨에 민감하여 날씨가 안 좋으면 페스티벌에 가지 않는다고 생각하겠습니다. 
            이런 종류의 의사 결정을 모델링하기 위해 퍼셉트론을 이용할 수 있습니다. 
            모델링하는 한 가지 방법으로 날씨에 관한 가중치 $w_1 = 6$으로 설정하고, 
            다른 조건에 관한 가중치를 각각 $w_2 = 2, w_3 = 2$로 고를 수 있습니다. 
            가중치 $w_1$의 값이 커질수록 당신에게 날씨가 이성친구의 동행여부와 대중교통의 편리성보다 중요하다는 것을 의미합니다. 
            마지막으로 퍼셉트론에 대한 역치를 $5$로 정했다고 가정하겠습니다. 
            이렇게 가중치를 선택하면 날씨가 좋으면 퍼셉트론 출력은 1이 되고 날씨가 나쁘면 0이 되어 우리가 원하는 의사 결정 모델을 구현하는 퍼셉트론을 얻을 수 있습니다. 
            이성친구가 함께 가고 싶은지, 대중 교통이 가까이 있는지는 출력에 영향을 주지 않습니다. 
          </p>

          <p>
            가중치와 역치를 바꿔가면서 다른 의사 결정 모델을 만들 수 있습니다. 
            예를 들어, 역치가 3이라고 가정하겠습니다. 
            그러면 날씨가 좋을 때 페스티벌에 가거나 이성친구가 함께 가고 대중교통이 가까이 있는 경우에 페스티벌에 가야하는 퍼셉트론이 됩니다. 
            즉, 다른 의사 결정 모델이 됩니다. 
            역치가 떨어지면 당신이 페스티벌에 가는 경우는 더 많아집니다. 
          </p>

          <p>
            퍼셉트론이 명백하게 완전한 의사 결정 모델은 아닙니다. 
            하지만 예시는 의사 결정을 하기위해 퍼셉트론이 어떻게 서로 다른 증거를 가늠하는지 보여줍니다. 
            그리고 복잡한 퍼셉트론 네트워크가 미묘한 의사 결정을 할 수 있는 것처럼 보입니다. 
            <br /><br />
            <img class="contentimg" src="./images/chapter1/tikz1.png" alt="digits" width="500px">
            <br /><br /> 
            네트워크에서 첫 번째 퍼셉트론 열은 퍼셉트론의 첫 번째 층이라고도 부릅니다. 
            네트워크에서 퍼셉트론의 첫 번째 열은 입력을 고려하여 세 개의 결정을 내립니다. 
            퍼셉트론의 두 번째 층은 어떨까요? 
            두 번째 층에 있는 각각의 퍼셉트론은 첫 번째 층의 의사결정 결과를 고려하여 결정을 내립니다. 
            이런 방식으로 두 번째 층에 있는 퍼셉트론은 첫 번째 층의 퍼셉트론보다 더 복잡하고 추상적인 결정을 내릴 수 있습니다. 
            그리고 훨씬 더 복잡한 결정은 세 번째 층에 있는 퍼셉트론이 할 수 있습니다. 
            이러한 방식으로 다층 퍼셉트론 네트워크는 수준 높은 의사 결정에 참여할 수 있습니다.
          </p>

          <p>
            퍼셉트론을 정의할 때 하나의 퍼셉트론은 하나의 출력을 가진다고 정의했습니다. 
            위의 그림에서 본 퍼셉트론 네트워크는 여러 개의 출력을 가지는 것처럼 보입니다. 
            사실 위의 퍼셉트론도 하나의 출력을 가집니다. 
            여러 개의 출력 화살표는 단지 하나의 퍼셉트론의 출력이 여러 개의 다른 퍼셉트론의 입력으로 사용될 수 있다는 것을 나타내는 유용한 방법입니다. 
            하나의 화살표가 나누어 지는 것을 그리는 것보다 편한 방법입니다.
          </p>

          <p>
            퍼셉트론을 나타내는 방법을 간단히 해보겠습니다. 
            조건 $\sum_j w_j x_j > \mbox{threshold}$을 간단하게 나타내기 위해 두 가지 표기를 바꿀 수 있습니다. 
            첫 번째 변화는 $\sum_j w_j x_j$을 내적으로 표현하는 겁니다. 
            내적으로 표현하면 $w \cdot x \equiv \sum_j w_j x_j$이고, $w$와 $x$는 성분이 각각 가중치와 입력인 벡터입니다. 
            두 번째 변화는 역치를 부등식의 좌변으로 옮겨 퍼셉트론의 바이어스 $b \equiv -\mbox{threshold}$로 치환하는 방법입니다. 
            역치 대신에 편향을 사용해서 퍼셉트론 규칙을 다시 쓰면 다음과 같습니다. 
            $$\begin{eqnarray}
            \mbox{output} = \left\{ 
              \begin{array}{ll} 
                0 & \mbox{if } w\cdot x + b \leq 0 \\
                1 & \mbox{if } w\cdot x + b > 0
              \end{array}
            \right.
            \tag{2}\end{eqnarray}$$
            그러면 편형을 퍼셉트론의 출력이 1이 되기 쉬운 정도로 볼 수 있습니다. 
            또는 생물학적인 용어를 사용하면 편향을 퍼셉트론이 활동하는게 얼마나 쉬운지 나타냅니다. 
            퍼셉트론이 큰 편향을 가지면 퍼셉트론의 출력이 1이되기 엄청 쉽습니다. 
            하지만 편향이 매우 작은 음수인 경우 퍼셉트론의 출력이 1이 되기 매우 어렵습니다. 
            퍼셉트론은 표현하는데 편향을 도입한 것은 분명히 작은 변화이지만 나중에 표기의 단순함을 이끌어 낼 것입니다. 
            이로 인해 책의 나머지 부분에서는 역치를 사용하지 않고 편향을 사용합니다.
          </p>

          <p>
            퍼셉트론은 결정을 내리기 위한 증거를 고려하는 방법입니다. 
            퍼셉트론을 이용하면 AND, OR, NANd와 같은 기초적인 논리 함수도 계산할 수 있습니다. 
            예를 들어, 두 개의 입력을 가지고 가중치가 각각 $-2$이고 편향이 $3$인 퍼셉트론이 있다고 가정해보겠습니다. 
            퍼셉트론을 그려보면 아래와 같습니다. 
            <br /><br />
            <img class="contentimg" src="./images/chapter1/tikz2.png" alt="digits" width="300px">
            <br /><br /> 
            그러면 입력이 $00$이면 $(-2)*0+(-2)*0+3 = 3$으로 양수이므로 출력은 $1$입니다. 
            여기서 $*$기호는 곱셈을 나타냅니다. 
            비슷한 방법으로 입력이 $01$이나 $10$이면 출력은 $1$입니다. 
            하지만 입력이 $11$인 경우 $(-2)*1+(-2)*1+3 = -1$은 음수이므로 출력은 $0$입니다. 
            그리고 이 퍼셉트론은 NAND 게이트입니다. 
          </p>

          <p>
            NAND 게이트 예시를 통해 퍼셉트론을 이용한 간단한 논리 함수를 계산을 보여주었습니다. 
            사실 퍼셉트론 네트워크를 이용하면 어떤 논리 함수도 구현할 수 있습니다. 
            NAND 게이트는 범용 게이트이므로 어떠한 계산도 NAND 게이트를 통해 할 수 있습니다. 
            예를 들어, NAND 게이트를 이용해서 두 개의 비트 $x_1, x_2$를 더하는 회로를 만들 수 있습니다. 
            이는 비트 합 $x_1 \oplus x_2$과 $x_1$과 $x_2$가 모두 1일 때 1이되는 자리 올림 비트가 필요합니다. 
            자리 올림 비트는 비트 곱 $x_1 x_2$으로 쓸 수 있습니다. 
            <br /><br />
            <img class="contentimg" src="./images/chapter1/tikz3.png" alt="digits" width="500px">
            <br /><br /> 
            동등한 퍼셉트론 네트워크를 구하려면 모든 NAND 게이트를 퍼셉트론으로 치환합니다. 
            각 퍼셉트론은 두 개의 입력을 가지고 가중치는 $-2$이며 전체 편향은 $3$입니다. 
            아래 결과 네트워크를 그렸습니다. 
            도표에서 화살표를 쉽게 그리기위해 오른쪽 아래의 NAND 게이트위치에 상응하는 퍼셉트론을 조금 옮겨 그렸습니다. 
            <br /><br />
            <img class="contentimg" src="./images/chapter1/tikz4.png" alt="digits" width="500px">
            <br /><br />
            위에 나타난 퍼셉트론 네트워크에서 주목할만한 점은 가장 왼쪽에 있는 퍼셉트론의 출력이 가장 아래 있는 퍼셉트론의 입력에 두 번 사용된다는 점입니다. 
            퍼셉트론을 정의할 때 출력 두 개가 중복으로 같은 입력으로 사용될 수 있는지 말하지 않았습니다. 
            사실 그렇게 중요한 문제는 아닙니다. 
            두 개의 출력선 각각은 가중치 $-2$를 가지기 때문에 두 개의 출력선을 하나로 합쳐 나타낸 뒤 가중치를 $-4$로 나타낼 수 있습니다. 
            퍼셉트론 네트워크를 아래처럼 다시 그릴 수 있습니다. 표시하지 않은 가중치는 $-2$이고, 편향은 모두 $3$입니다. 하나의 가중치 $-4$는 표시하였습니다. 
            <br /><br />
            <img class="contentimg" src="./images/chapter1/tikz5.png" alt="digits" width="500px">
            <br /><br />
            지금까지 입력 변수 $x_1, x_2$를 퍼셉트론 네트워크 왼편에 표시했습니다. 
            사실 입력을 인코딩하는 입력 층을 그리는 것이 관습입니다. 
            <br /><br />
            <img class="contentimg" src="./images/chapter1/tikz6.png" alt="digits" width="500px">
            <br /><br />
            입력 퍼셉트론에 대한 표기법으로 출력은 그리고, 입력은 없어도 됩니다. 
            <br /><br />
            <img class="contentimg" src="./images/chapter1/tikz7.png" alt="digits" width="150px">
            <br /><br />
            입력이 없는 퍼셉트론을 의미하는 것은 아닙니다. 
            입력이 없는 퍼셉트론이라고 가정하면 가중치 합 $\sum_j w_j x_j$은 항상 0이 되고, $b > 0$이면 출력은 1 $b \leq 0$이면 출력은 0이 됩니다. 
            즉 이 퍼셉트론의 출력은 고정된 값을 가지고 원하는 출력 값(위 예에서 $x_1$)을 얻을 수 없습니다. 
            입력 퍼셉트론은 퍼셉트론이 아니라 원하는 출력 값 $x_1, x_2,\ldots$을 가지는 특별한 단위로 생각하면 됩니다. 
          </p>

          <p>
            덧셈기 예제는 퍼셉트론 네트워크가 많은 수의 NAND 게이트를 포함한 회로에 어떻게 사용되는지 보여줍니다. 
            그리고 NAND 게이트는 범용 게이트이므로 퍼셉트론 또한 범용적으로 사용할 수 있습니다. 
          </p>

          <p>
            퍼셉트론의 범용성은 다행스럽기도 하고 실망스럽기도 합니다.  
            다행인 점은 퍼셉트론 네트워크가 어떤 계산 장치보다 강력한 기능을 할 수 있습니다. 
            하지만 퍼셉트론이 단시 새로운 형태의 NAND 게이트라는 점은 실망스럽습니다. 이는 전혀 새로운 뉴스가 아닙니다. 
          </p>

          <p>
            하지만 이러한 관점이 제시하는 상황은 많이 달라졌습니다. 
            인공 뉴런 네트워크의 가중치와 편향 값을 자동으로 조정하는 학습 알고리즘을 고안할 수 있다는 사실이 밝혀졌습니다. 
            이러한 조정은 프로그래머가 아닌 외부 자극에 대한 반응으로 일어납니다. 
            학습 알고리즘 덕분에 우리는 인공 뉴런을 전통적인 논리 게이트와 다른 방식으로 사용할 수 있습니다. 
            NAND와 다른 게이트를 사용한 회로 대신에 뉴럴 네트워크는 문제를 풀기 위해 학습을 할 수 있습니다. 
            때로는 직접 회로를 설계하기 어려운 문제들도 풀 수 있습니다. 
          </p>

          <br />
          <h2>시그모이드 뉴런</h2>
          <hr>

          <p>
            학습 알고리즘이라는 말은 굉장하게 들립니다. 
            그렇다면 뉴럴 네트워크를 위한 학습 알고리즘을 어떻게 설계할 수 있을까요? 
            물제를 풀기 위해 학습에 사용하고 싶은 퍼셉트론 네트워크가 있다고 가정하겠습니다. 
            예를 들어 네트워크에 대한 입력은 손글씨 숫자 이미지를 스캔해서 얻은 가공되지 않은 픽셀 데이터라고 하겠습니다. 
            그리고 우리는 네트워크가 가중치와 편향을 학습하여 네트워크의 출력이 숫자를 올바르게 분류하기를 원합니다. 
            네트워크의 가중치 혹은 편향을 조금씩 바꾼다고 가정하여 어떻게 학습 과정이 일어나는지 살펴보겠습니다. 
            우리가 하고 싶은 것은 가중치의 작은 변화가 네트워크 출력의 작은 변화를 이끈다는 것입니다. 
            잠시 후 알게 되겠지만 이런 속성때문에 학습이 가능합니다. 
            도표로 나타내면 아래와 같습니다.(아래 네트워크는 너무 단순하여 손글씨를 인식하지는 못합니다.) 
            <br /><br />
            <img class="contentimg" src="./images/chapter1/tikz8.png" alt="digits" width="500px">
            <br /><br />
            가중치(또는 편향)가 조금 변할 때 출력도 조금 변한다면 우리는 이 사실을 이용해 가중치와 편향을 수정하면서 원하는 방식으로 네트워크가 동작하도록 만들 수 있습니다. 
            예를 들어, 네트워크가 숫자 "9" 이미지를 숫자 "8"로 잘못 인식한다고 가정해 보겠습니다. 
            그러면 우리는 네트워크가 이미지를 숫자 "9"로 분류하도록 어떻게 가중치와 편향을 조금씩 바꿀지 알아내야 합니다. 
            더 좋은 결과를 얻도록 가중치와 편향을 반복해서 바꾸어 나가야 합니다. 
            이렇게 네트워크는 학습을 진행합니다.
          </p>

          <p>
            문제는 퍼셉트론으로 구성된 네트워크에서는 학습 과정이 일어나지 않습니다. 
            사실 어떤 하나의 퍼셉트론의 가중치 혹은 편향이 조금 변할 때 출력은 0에서 1로 바뀌는 것처럼 완전히 뒤집어 지는 경우가 있습니다. 
            출력이 완전히 뒤바뀌면 네트워크의 나머지 부분도 복잡한 방식으로 완전히 변합니다. 
            그래서 숫자 "9"는 이제 정확히 분류하지만 다른 이미지들의 결과는 통제할 수 없을 정도로 완전히 바뀔 수 있습니다. 
            이러한 이유때문에 얼마만큼 가중치와 편향을 수정해야 네트워크가 원하는 행동에 가까워지는지 알기어렵습니다. 
            아마도 이 문제를 해결하는 다른 현명한 방법이 있을 것입니다. 
            하지만 퍼셉트론 네트워크가 어떻게 학습하는지 명확하게 알기 어렵습니다. 
          </p>

          <p>
            이 문제를 해결하기 위해 시그모이드 뉴런이라는 새로운 형태의 인공 뉴런을 도입하겠습니다. 
            시그모이드 뉴런은 퍼셉트론과 비슷하지만 가중치와 편향이 조금 변할때 출력도 조금 변합니다. 
            이 중요한 사실때문에 시그모이드 뉴런은 학습을 할 수 있습니다. 
          </p>

          <P>
            이제 시그모이드 뉴런을 설명하겠습니다. 
            퍼셉트론을 그렸던 방식처럼 시그모이드 뉴런을 그릴 수 있습니다. 
            <br /><br />
            <img class="contentimg" src="./images/chapter1/tikz9.png" alt="digits" width="300px">
            <br /><br />
            퍼셉트론처럼 시그모이드 뉴런은 입력 $x_1, x_2, \ldots$을 가집니다. 
            대신 입력 값은 $0$ 또는 $1$이 아니라 $0$과 $1$ 사이의 값을 가질 수 있습니다. 
            그래서 예를 들어 $0.638\ldots$ 같은 값은 시그모이드 뉴런의 입력이 될 수 있습니다. 
            또한 퍼셉트론처럼 시그모이드 뉴런도 각 입력에 대한 가중치 $w_1, w_2, \ldots$과 전체 편향 $b$를 가집니다. 
            하지만 출력은 $0$ 또는 $1$이 아닙니다. 
            출력은 $\sigma(w \cdot x+b)$입니다. 
            여기서 $\sigma$는 시그모이드 함수
            <span class="tooltip"><b style="font-size: 20px">*</b>
              <span class="tooltiptext">
                $\sigma$는 로지스틱 함수라고도 부르고, 
                이 뉴런을 로지스틱 뉴런이라 합니다. <br />
                이 용어를 기억해두면 유용합니다. 
                뉴럴 네트워크를 이용하는 많은 사람이 이 용어를 사용하기 때문입니다. <br />
                하지만 이 책에서는 시그모이드라는 용어를 사용하겠습니다.
              </span>
            </span>
            라 부르고 다음과 같이 정의합니다. 
            $$\begin{eqnarray} 
            \sigma(z) \equiv \frac{1}{1+e^{-z}}.
            \tag{3}\end{eqnarray}$$
            조금 더 자세하게 식을 써보겠습니다. 
            입력이 $x_1,x_2,\ldots$, 가중치 $w_1,w_2,\ldots$, 편향 $b$일 때 시그모이드 뉴런의 출력은 다음과 같습니다. 
            $$\begin{eqnarray} 
            \frac{1}{1+\exp(-\sum_j w_j x_j-b)}.
            \tag{4}\end{eqnarray}$$
            한 눈에 보면 퍼셉트론과 시그모이드 뉴런의 차이가 커보입니다. 
            수학적 표현에 익숙하지 않으면 이해하기 어렵고 거부감이 있을 수 있습니다. 
            사실 퍼셉트론과 시그모이드 뉴런 사이에는 비슷한 점이 많습니다. 
            수학적으로 이해하는데 장벽이 있는 시그모이드 함수의 수학적 표현은 기술적으로 자세한 부분을 더 많이 알려줍니다. 
          </P>

          <p>
            퍼셉트론 모델과 비슷한 점을 이해하기 위해 $z \equiv w \cdot x + b$은 큰 양수라고 가정해봅시다. 
            그러면 $e^{-z} \approx 0$이 되고 $\sigma(z) \approx 1$이 됩니다. 
            즉, $z = w \cdot x+b$이 아주 큰 양수일 때 시그모이드 뉴런의 출력은 대략 1이 됩니다. 
            퍼셉트론의 경우에도 결과는 같습니다. 
            $z = w \cdot x+b$이 아주 작은 음수라고 가정해봅시다. 
            그러면 $e^{-z} \rightarrow \infty$이고 $\sigma(z) \approx 0$이 됩니다. 
            그래서 $z = w \cdot x+b$이 아주 작은 음수일 때 시그모이드 뉴런의 행동은 대략적으로 퍼셉트론과 비슷합니다. 
            단지 $w \cdot x+b$이 적당한 값을 가질 때만 퍼셉트론 모델과의 차이를 보입니다. 
          </p>

          <p>
            $\sigma$의 수학적 표현은 어떤가요? 
            우리는 그것을 어떻게 이해면 될까요? 
            사실 $\sigma$의 수학적 표현 자체는 크게 중요하지 않습니다. 
            중요한 것은 함수를 그래프를 그렸을 때의 모양입니다. 
            아래 그래프가 있습니다. 
            <br /><br />
            <img class="contentimg" src="./images/chapter1/graph-ch1-01.png" alt="digits" width="500px">
            <br /><br />
            아래의 그래프는 계단 함수의 그래프입니다. 
            <br /><br />
            <img class="contentimg" src="./images/chapter1/graph-ch1-02.png" alt="digits" width="500px">
            <br /><br />
            $\sigma$가 계산 함수라면 시그모이드 뉴런은 $w\cdot x+b$이 양수냐 음수냐
            <span class="tooltip"><b style="font-size: 20px">*</b>
              <span class="tooltiptext">
                사실 $w \cdot x +b = 0$일 때 퍼셉트론의 출력은 $0$이고, 계단 함수의 출력은 $1$입니다. <br />
                그래서 엄밀히 따지자면 계단 함수를 수정할 필요가 있습니다. <br />
                하지만 아이디어를 얻을 수 있습니다. 
              </span>
            </span>
            에 따라 출력이 1 또는 0이 되므로 퍼셉트론이 될 것입니다. 
            실제 $\sigma$ 함수를 사용하여 우리는 위에서 살펴본 퍼셉트론을 얻을 수 있습니다. 
            $\sigma$ 함수의 수학적 표현보다 부드러운 곡선 형태가 정말 중요합니다. 
            $\sigma$ 함수의 부드러운 곡선 형태가 의미하는 것은 뉴런에서 가중치의 아주 작은 변화 $\Delta w_j$와 편향의 아주 작은 변화 $\Delta b$에 의해 출력의 아주 작은 변화 $\Delta \mbox{output}$을 얻는다는 것입니다. 
            사실 미분을 이용하면 $\Delta \mbox{output}$을 다음과 같이 쓸 수 있습니다. 
            $$\begin{eqnarray} 
            \Delta \mbox{output} \approx \sum_j \frac{\partial \, \mbox{output}}{\partial w_j}
            \Delta w_j + \frac{\partial \, \mbox{output}}{\partial b} \Delta b,
            \tag{5}\end{eqnarray}$$
            여기서 합은 모든 가중치 $w_j$에 대한 것이고, 
            $\partial \, \mbox{output} / \partial w_j$과 $\partial \, \mbox{output} /\partial b$은 
            각각 $w_j$과 $b$에 대한 출력의 편미분을 의미합니다. 
            편미분을 모르더라도 괜찮습니다. 
            편미분 때문에 위의 식이 복잡해 보이지만 사실 말하고자 하는 것은 아주 간단합니다. 
            $\Delta \mbox{output}$은 가중치와 편향에서의 변화 $\Delta w_j$와 $\Delta b$에 대한 선형 함수입니다. 
            이러한 선형성 때문에 출력에서 원하는 아주 작은 변화를 만들기 위해 가중치와 편향에서 아주 작은 변화를 고르는 것이 쉽습니다. 
            그래서 시그모이드 뉴런이 퍼셉트론과 같은 행동을 보이지만 시그모이드 뉴런은 출력을 바꾸기 위해 가중치와 편향을 얼마만큼 바꿔야 하는지 알기 쉽습니다. 
          </p>

          <p>
            $\sigma$ 함수의 수학적 표현보다 그래프 모양이 중요하다면 방정식
            <span class="tooltip"><b>(3)</b>
              <span class="tooltiptext">
                $$\begin{eqnarray} 
                \sigma(z) \equiv \frac{1}{1+e^{-z}} \nonumber\end{eqnarray}$$
              </span>
            </span>
            과 같은 $\sigma$의 수학적 표현을 사용하는 이유는 무엇일까요? 
            사실 책의 뒷 부분에서 다른 활성화 함수 $f(\cdot)$에 대한 출력이 $f(w \cdot x + b)$인 뉴런을 언급할 것입니다. 
            다른 활성화 함수를 사용할 때 주된 변화는 방정식
            <span class="tooltip"><b>(5)</b>
              <span class="tooltiptext">
                $$\begin{eqnarray} 
                \Delta \mbox{output} \approx \sum_j \frac{\partial \, \mbox{output}}{\partial w_j}
                \Delta w_j + \frac{\partial \, \mbox{output}}{\partial b} \Delta b \nonumber\end{eqnarray}$$
              </span>
            </span>
            에서 나타나는 편미분 값의 변화입니다. 
            지수함수의 미분은 몇몇 특성을 지니므로 나중에 편미분을 계산할 때  $\sigma$를 사용하면 계산을 간단히 할 수 있습니다. 
            어쨋든 $\sigma$는 뉴럴 네트워크에서 흔하게 사용되고, 이 책의 대부분에서 사용되는 활성화 함수입니다. 
          </p>

          <p>
            시그모이드 뉴런의 출력은 어떻게 해석하면 될까요? 
            시그모이드 뉴런의 출력은 단지 0 또는 1이 아니라는 점이 분명 퍼셉트론과 시그모이드 뉴런의 큰 차이입니다. 
            시그모이드 뉴런의 출력은 $0$과 $1$사이의 어떤 실수도 될 수 있습니다. 
            따라서 $0.173\ldots$과 $0.689\ldots$같은 값도 출력이 될 수 있습니다. 
            예를 들어 시그모이드 뉴런의 출력을 이용해서 뉴럴 네트워크의 입력 이미지 픽셀의 평균 명암도를 나타낼 수 있습니다. 
            하지만 골칫거리일 수 있습니다. 
            네트워크의 출력이 입력 이미지가 "9"인지 아닌지를 나타내고 싶다고 생각해봅시다. 
            분명히 퍼셉트론처럼 출력이 $0$ 또는 $1$이면 굉장히 쉽습니다. 
            하지만 실제로 이를 다루기 위해서는 규칙을 정해야 합니다. 
            예를 들어 출력이 $0.5$ 이상이면 "9"로 해석하고, 출력이 $0.5$ 보다 작으면 "9가 아니다"라고 해석합니다. 
            책에서는 혼란을 주지 않기 위해 이런 규칙을 명확히 정의해 놓았습니다. 
          </p>

          <br />
          <h2>예제</h2>

          <p>
            <ul>
              <li>시그모이드 뉴런 파트 1</li>
              <p>
                블라블라
              </p>
              <li>시그모이드 뉴런 파트 2</li>
            </ul>
          </p>

          <br />
          <h2>뉴럴 네트워크의 구조</h2>
          <hr>

          <p>
            이번 절에서는 손 글씨 숫자 분류 작업 성능이 좋은 뉴럴 네트워크를 소개하겠습니다.  
            먼저 네트워크의 서로 다른 부분을 부르는 용어에 대해 설명하겠습니다. 
            아래와 같은 네트워크가 있다고 가정해봅시다. 
            <br /><br />
            <img class="contentimg" src="./images/chapter1/tikz10.png" alt="digits" width="400px">
            <br /><br />
            앞서 살펴본 것 처럼 네트워크에서 가장 왼쪽에 있는 층은 입력 층이라 부릅니다. 
            입력 층에 있는 뉴런은 입력 뉴런이라 부릅니다. 
            가장 왼쪽에 있는 출력 층은 출력 뉴런을 가지고 있습니다. 
            위의 예에서는 하나의 출력 뉴런을 가지고 있습니다. 
            가운데 있는 층은 은닉 층이라 부릅니다. 
            은닉 층에 있는 뉴런은 입력도 아니고 출력도 아니기 때문이 이런 이름이 붙었습니다. 
            "은닉"이라는 용어가 이상하게 들리지도 모릅니다. 
            저는 처음 이 용어를 들었을 때 어떤 심오한 철학이나 수학적 중요성이 있을 것이라 생각했습니다. 
            하지만 "입력도 아니고 출력도 아니다"라는 의미 이외에 다른 뜻은 없습니다. 
            위의 네트워크는 하나의 은닉 층을 가지고 있지만 어떤 네트워크는 다수의 은닉 층을 가집니다. 
            예를 들어 아래의 네 개 층으로 구성된 네트워크는 두 개의 은닉층을 가집니다. 
            <br /><br />
            <img class="contentimg" src="./images/chapter1/tikz11.png" alt="digits" width="700px">
            <br /><br />
            다소 헷갈릴 수 있지만 역사적인 이유 때문에 다층 네트워크를 퍼셉트론이 아닌 시그모이드 뉴런으로 이루어졌다 할지라도 종종 다층 퍼셉트론이라 부르기도 합니다. 
            이 책에서는 다층 퍼셉트론이라는 말을 쓰지 않을 것입니다. 
            헷갈릴 수 있는 용어라고 생각하기 때문입니다. 
            하지만 이런 용어가 있다는 것을 알아두세요. 
          </p>

          <p>
            네트워크에서 입력층과 출력층의 설계는 간단합니다. 
            예를 들어 손글씨 이미지가 "9"인지 아니지 결정하고 싶다고 합시다. 
            네트워크를 설계하는 자연스러운 방법은 이미지 픽셀의 명암을 입력 뉴런으로 인코딩하는 것입니다. 
            이미지가 $64 \times 64$ 그레이스케일 이미지라면 $0$에서 $1$ 사이 값을 명암을 가지는 입력 뉴런 $4,096 = 64 \times 64$개를 가지면 됩니다. 
            출력층은 입력 이미지가 "9"가 아니면 $0.5$보다 작은 값을 출력으로 하고, "9"가 맞으면 $0.5$보다 큰 값을 출력으로 하는 하나의 뉴런을 가지면 됩니다. 
          </p>

          <p>
            뉴럴 네트워크의 입력과 출력층의 설계는 간단하지만 은닉 층의 설계는 상당한 기술이 필요합니다. 
            특히 은닉 층의 설계 과정을 간단한 규칙으로 설명하는 것은 불가능합니다. 
            대신 뉴럴 네트워크 연구자들은 은닉 층에 대해 많은 경험에서 우러나온 설계 방법을 개발했습니다. 
            이를 통해 뉴럴 네트워크에서 원하는 행동을 얻을 수 있습니다. 
            예를 들어 이런 방법을 통해 은닉 층 개수와 네트워크를 학습시키는 시간 사이의 균형을 유지할 수 있습니다. 
            이 책의 뒷부분에서 이런 설계 방식에 대해 살펴보겠습니다. 
          </p>

          <p>
            지금까지 한 층에서 나온 출력이 다른 층의 입력으로 사용되는 뉴럴 네트워크에 대해 살펴보았습니다. 
            이런 네트워크를 피드포워드 뉴럴 네트워크라고 부릅니다. 
            이는 네트워크에 루프가 없다는 것을 의미합니다. 
            정보는 항상 앞으로 이동하고 뒤로 돌아오는 피드백 과정은 없습니다. 
            네트워크에 루프가 있다면 $\sigma$ 함수의 입력이 출력에 따라 바뀌는 상황을 마주합니다. 
            이는 이해하기 어려운 일이기 때문에 루프를 제외했습니다.
          </p>

          <p>
            하지만 피드백 루프가 존재하는 인공 뉴럴 네트워크 모델이 있습니다. 
            이런 모델을 <a href="https://ko.wikipedia.org/wiki/%EC%88%9C%ED%99%98_%EC%8B%A0%EA%B2%BD%EB%A7%9D">리커런트 뉴럴 네트워크</a>라고 합니다. 
            이 모델은 휴지 상태가 되기 전에 일정 시간동안 활동 상태가 지속되는 뉴런을 아이디어로 하고 있습니다. 
            뉴런이 활동전위가 되면 다른 뉴런을 자극하여 약간의 시간이 흐른 뒤 활동전위가 됩니다. 
            그리고 시간이 지나면서 연속적으로 다른 뉴런들도 활동전위가 됩니다. 
            루프는 이런 모델에서 아무 문제가 없습니다. 
            뉴런의 출력이 즉시 입력에 영향을 주는 것이 아니라 약간의 시간이 지난 뒤 영향을 주기 때문입니다.  
          </p>

          <p>
            리커런트 뉴럴 네트워크는 피드 포워드 네트워크보다 영향력이 크지 않습니다. 
            현재까지는 리커런트 뉴럴 네트워크의 학습알고리즘이 강력하지 않기 때문입니다. 
            하지만 리커런트 뉴럴 네트워크는 매우 흥미롭습니다. 
            리커런트 뉴럴 네트워크는 피드 포워드 네트워크보다 인간 두뇌 동작방식과 유사합니다. 
            그리고 피드 포워드 네트워크에서는 풀기 어려운 문제가 리커런트 뉴럴 네트워크가 풀 수 있습니다. 
            하지만 범위를 제한해서 이 책에서는 더 많이 사용되는 피드 포워드 네트워크에 집중하겠습니다. 
          </p>

          <br />
          <h2>손글씨 숫자를 분류하는 간단한 뉴럴 네트워크</h2>
          <hr>

          <p>
            뉴럴 네트워크를 정의했으니 다시 손글씨 인식 문제로 돌아가 봅시다. 
            손글씨 숫자 인식 문제를 두 개의 문제로 나눠 볼 수 있습니다. 
            먼저 여러 개의 숫자를 포함하는 이미지를 하나의 숫자를 포함하는 별개의 이미지로 나눕니다. 
            예를 들어 아래의 이미지를 6개의 이미지로 나눕니다. 
            <br /><br />
            <img class="contentimg" src="./images/chapter1/digits.png" alt="digits" width="300px">
            <br /><br />

            <br /><br />
            <img class="contentimg" src="./images/chapter1/digits_separate.png" alt="digits" width="400px">
            <br /><br />
            인간은 이 세그멘테이션 문제를 쉽게 풀 수 있지만 컴퓨터 프로그램이 풀기에는 어렵습니다. 
            이미지를 구분 한 뒤 프로그램은 각 숫자를 분류합니다. 
            예를 들어 위에서 프로그램이 첫 번째 숫자를 인식하면 5입니다.
            <br /><br />
            <img class="contentimg" src="./images/chapter1/mnist_first_digit.png" alt="digits" width="50px">
            <br /><br />
          </p>

          <p>
            이제 각 숫자를 분류하는 프로그램을 작성하는 법을 살펴보겠습니다. 
            일단 개별 숫자를 분류하는 좋은 방법이 있다면 분할 문제는 그렇게 풀기 어려운 문제는 아닙니다. 
            분할 문제는 푸는 방법은 여러가지가 있습니다. 
            한 가지 방법은 이미지를 분할하는 서로 다른 많은 시도를 하고 개별 숫자 분류기를 이용하여 각 분할에 대해 점수를 매기는 것입니다. 
            모든 분할에 대해 숫자 분류기를 신뢰할 수 있으면 분할 시도는 높은 점수를 받습니다. 
            분류기가 하나 또는 그 보다 많은 분할에서 문제를 가지면 낮은 점수를 받습니다. 
            분류기가 어딘가에서 문제를 가지면 분할이 잘못 선택되었기에 문제가 발생한다는 아이디어입니다. 
            이 아이디어와 다른 변형들이 사용되어 분할 문제를 잘 풀 수 있습니다. 
            그래서 분할 문제가 아닌 더 흥미롭고 어려운 문제인 손글씨 숫자 인식 문제를 풀 수 있는 뉴럴 네트워크 개발에 집중할 것입니다.  
          </p>

          <p>
            각 숫자를 인식하기 위해 삼중층 뉴럴 네트워크를 사용하겠습니다. 
            <br /><br />
            <img class="contentimg" src="./images/chapter1/tikz12.png" alt="digits" width="600px">
            <br /><br />
            뉴럴 네트워크의 입력층은 입력 픽셀 값을 인코딩하는 뉴런을 포함한다. 
            다음 절에서 살펴보겠지만 뉴럴 네트워크의 학습 데이터는 손글씨를 스캔한 28x28 픽셀 이미지로 구성됩니다. 
            그리고 입력층은 $784 = 28 \times 28$개 뉴런을 포함합니다. 
            간단하게 그리기 위해 위의 다이어그램에서는 784개 뉴런 중 일부만 그렸습니다. 
            입력 픽셀은 그레이스케일이고 $0.0$의 값은 흰색을 $1.0$은 검은색을 나타냅니다. 
            그리고 이 사이값은 점점더 어두워지는 흑색을 나타냅니다. 
          </p>

          <p>
            뉴럴 네트워크의 두 번째 층은 은닉층입니다. 
            은닉층에 있는 뉴런의 개수를 $n$이라 하고 $n$의 수를 달리하며 실험하겠습니다. 
            예시 그림에서 은닉층은 $n=15$개의 뉴런만 포함하고 있습니다. 
          </p>

          <p>
            뉴럴 네트워크의 출력층은 10개의 뉴런을 포함합니다. 
            첫 번째 뉴런이 활성화되어 출력으로 1을 가진다면 뉴럴 네트워크가 숫자를 0으로 분류했다는 것을 의미합니다. 
            두 번째 뉴런이 활성화되면 뉴럴 네트워크가 숫자를 1로 분류했다는 의미입니다. 
            세 번째 뉴런, 그 이후의 뉴런에도 마찬가지로 적용하면 됩니다. 
            조금 더 정확하게 출력 뉴런을 $0$에서 $9$까지 번호를 매기고 어느 뉴런의 활성 값이 가장 높은지 알아냅니다. 
            예를 들어 $6$번 뉴런이 활성화된다면 신경망은 입력 숫자가 $6$이라고 판단합니다. 
            다른 출력 뉴런에 대해서도 마찬가지입니다. 
          </p>

          <p>
            왜 $10$개의 뉴런을 사용하는지 궁금할 수 있습니다. 
            무엇보다도 신경망의 목표는 숫자 $0, 1, 2, \ldots, 9$에 대응하는 입력 이미지를 판별하는 것입니다. 
            겉보기에는 뉴런의 출력이 $0$ 또는 $1$인지에 따라 이진 값을 가지도록 하는 4개의 출력뉴런을 사용하면 될 것 같습니다. 
            4개의 뉴런은 $2^4 = 16$으로 10개의 가능한 입력 숫자 값을 가지므로 답을 인코딩하기에 충분합니다. 
            하지만 왜 이 신경망에서 $10$개의 뉴런을 사용할까요? 
            비효율적이지 않나요? 
            타당한 이유는 경험적인 바탕에 있습니다. 
            두 가지 신경망을 설계해서 실험할 수 있고, 이 특별한 문제에 대해서는 4개의 출력 뉴런을 가지는 신경망보다 10개의 출력 뉴런을 가지는 신경망이 숫자 인식을 더 잘 학습합니다. 
            하지만 여전히 왜 10개의 출력 뉴런을 가지는 것이 더 잘 동작하는지 의문이 남아있습니다. 
            4개 출력을 인코딩하는 대신에 10개 출력을 인코딩하는 것을 미리 알 수 있는 방법은 있을까요? 
          </p>

          <p>
            우리가 이것을 왜 하는지 이해하하는데 신경망은 첫번째 원칙에서 무엇을 하는지 생각하는 것이 도움을 줄 수 있습니다. 
            먼저 10개의 출력 뉴런을 사용하는 경우를 생각해봅시다. 
            첫 번째 출력 뉴런을 보면 숫자가 0인지 아닌지 결정하기 위한 뉴런입니다. 
            은닉층의 증거를 가늠하여 이를 결정합니다. 
            은닉층 뉴런은 무엇을 할까요? 
            은닉층에 있는 첫 번째 뉴런이 아래와 같은 이미지가 있는지 없는지 판단한다고 가정해봅시다. 
            <br /><br />
            <img class="contentimg" src="./images/chapter1/mnist_top_left_feature.png" alt="digits" width="130px">
            <br /><br />
            이미지와 겹치는 입력 픽셀은 가중치를 많이 두고 다른 입력은 가중치를 적게 두어 이를 판변할 수 있습니다. 
            비슷한 방법으로 은닉층의 두 번째, 세 번째, 네번째 뉴런이 아래의 이미지가 있는지 판단한다고 가정해봅시다. 
            <br /><br />
            <img class="contentimg" src="./images/chapter1/mnist_other_features.png" alt="digits" width="450px">
            <br /><br />
            추측하다시피 위 네 개의 이미지는 처음에 보았던 숫자들 중 숫자 $0$ 이미지를 만듭니다. 
            <br /><br />
            <img class="contentimg" src="./images/chapter1/mnist_complete_zero.png" alt="digits" width="130px">
            <br /><br />
            그래서 은닉층에 있는 네 개의 뉴런이 모두 활성화되면 숫자가 $0$이라고 결론지을 수 있습니다. 
            물론 이미지가 $0$이라고 결론지을 수 있는 유일한 증거는 아닙니다. 
            다른 많은 방법으로도 $0$이라는 결정을 내릴 수 있습니다. 
            (가령 위 이미지의 번역이나 약간의 왜곡을 통해 알 수 있습니다.) 
            하지만 적어도 이 경우에는 입력이 $0$이라고 결론지을 수 있습니다. 
          </p>

          <p>
            신경망이 이런 방법으로 기능한다고 가정하면 4개의 출력보다 10개의 출력 뉴런을 가지는게 왜 좋은지 설명할 수 있습니다. 
            네 개의 출력을 가진다면 첫 번째 뉴런은 숫자 비트 중 어느 것이 가장 중요한지 결정하려 할 것입니다. 
            숫자에서 가장 중요한 비트를 위에서 본 간단한 모양과 관련짓는 쉬운 방법은 없습니다. 
            숫자의 모양과 출력에서 중요한 비트 사이에 밀접한 관련이 있다는 역사적 사실을 상상하기는 어렵습니다. 
          </p>

          <p>
            모두 말했듯이 이것은 경험적 발견입니다. 
            삼중층 신경망이 간단한 모양을 은닉층에 있는 뉴런이 가지하면서 제가 설명한 방법으로 동작해야한다는 것은 아닙니다. 
            아마도 더 현명한 학습알고리즘은 가중치 값을 찾아 단지 4개의 출력 뉴런만을 사용하는 방법을 알려줄 것입니다. 
            하지만 제가 설명한 경험적 발견 법칙은 잘 동작하기 때문에 좋은 신경망 구조를 설계하는데 많은 시간을 아낄 수 있습니다. 
          </p>

          <br />
          <h2>예제</h2>
          <hr>

          <p>
            <ul>
              <li>
                위에서 설명한 삼중층에 추가적인 층을 더해 숫자의 비트 표현을 결정하는 방법이 있습니다. 
                추가적인 층은 이전 층의 출력을 이진법으로 변환합니다. 
                아래의 그림을 보세요. 
                새로운 출력 층의 가중치와 편향을 찾으세요. 
                처음 세개 층의 뉴런은 다음과 같은 설명을 가진다고 가정합니다. 
                세 번째 층의 올바른 출력은 적어도 $0.99$ 이상 활성화되고 
                올바르지 않은 출력은 $0.01$ 이하 활성화됩니다. 
              </li>
            </ul>
            <br /><br />
            <img class="contentimg" src="./images/chapter1/tikz13.png" alt="digits" width="700px">
            <br /><br />
          </p>

          <br />
          <h2>경사 하강으로 학습하기</h2>
          <hr>

          <p>
            이제 신경망을 설계하였으니 신경망이 숫자를 인식하기 위해 어떻게 학습을 할까요? 
            먼저 필요한 것은 학습하기 위한 데이터 셋입니다. 
            이 데이터 셋은 학습 데이터 셋이라고도 부릅니다. 
            여기서 MNIST 데이터 셋을 사용하겠습니다. 
            MNIST 데이터 셋은 수만개의 스캔한 손글씨 숫자를 포함하고 있고 숫자의 올바른 분류를 가지고 있습니다. 
            MNIST는 NIST(미국표준기술연구소)가 수집한 두 데이터 셋의 수정된 부분집합이라는 의미입니다. 
            아래 MNIST 데이터 셋 중 일부를 가져왔습니다.
            <br /><br />
            <img class="contentimg" src="./images/chapter1/digits_separate.png" alt="digits" width="500px">
            <br /><br />
            알다시피 이 숫자는 사실 이 장의 처음 부분에 보여주었던 것과 같습니다. 
            물론 신경망을 테스트할 때 학습 데이터 셋이 아닌 이미지를 인식하도록 합니다. 
          </p>

          <p>
            MNIST 데이터 셋은 두 부분으로 나뉩니다. 
            첫 번째 부분은 학습 데이터로 사용되는 60,000개의 이미지를 포함합니다. 
            이 이미지는 250명의 사람들의 손글씨를 스캔한 것입니다. 
            이들 중 절반은 미국 통계국 직원이며 나머지 절반은 고등학생입니다. 
            이미지는 흑백이며 28x28 픽셀 크기입니다. 
            MNIST 데이터 셋의 두 번째 부분은 테스트 데이터로 사용되는 10,000개의 이미지입니다. 
            이 이미지도 28x28 크기의 흑백 이미지입니다. 
            우리는 테스트 데이터를 사용하여 신경망이 숫자 인식을 하기위해 학습을 얼마나 잘 하였는지 평가합니다. 
            좋은 테스트 성능을 만들기 위해 테스트 데이터는 기존 학습 데이터와 다른 250명의 사람들에서 가져왔습니다. 
            (마찬가지로 250명 중 절반은 미국 통계국 직원이고 나머지 절반은 고등학생입니다.) 
            이는 신경망이 학습하는 동안 보지 못한 데이터를 인식할 수 있다는 신뢰도를 높여줍니다.  
          </p>

          <p>
            $x$를 학습 데이터 입력이라고 해봅시다. 
            각 학습 데이터 입력 $x$를 28 $\times 28 = 784$ 차원 벡터로 생각하면 편리합니다. 
            벡터의 각 성분은 이미지에서 하나의 픽셀에 대한 흑백 값을 나타냅니다. 
            대응하는 원하는 출력을 $y = y(x)$이라 표시합니다. 
            여기서 $y$는 $10$ 차원 벡터입니다. 
            예를 들어 학습 이미지 $x$가 $6$을 나타낸다면 $y(x) = (0, 0, 0, 0, 0, 0, 1, 0, 0, 0)^T$은 신경망으로 부터 얻을 수 있는 출력입니다. 
            여기서 $T$는 전치행렬이며 행 벡터를 열 벡터로 바꿉니다. 
          </p>

          <p>
            우리가 하고자 하는 바는 알고리즘이 가중치와 편향을 찾아 신경망의 결과가 모든 학습 데이터 입력 $x$에 대해 $y(x)$에 근사하는 것이다. 
            이 목표에 얼마나 잘 도달했는지 수량화하기위해 비용함수
            <span class="tooltip"><b>*</b>
              <span class="tooltiptext">
                비용함수는 손실함수 또는 목적함수라고도 한다. <br />
                이 책에서는 비용함수라는 용어를 사용하지만  <br />
                신경망에 대한 연구 논문에서 다른 용어들이 사용되므로 알아둘 필요가 있다. <br />
              </span>
            </span>
            를 정의한다. 
            $$\begin{eqnarray}  C(w,b) \equiv
            \frac{1}{2n} \sum_x \| y(x) - a\|^2.
            \tag{6}\end{eqnarray}$$
            여기서 $w$는 신경망에서 모든 가중치를 의미하고 $b$는 모든 편향, $n$은 학습 데이터 입력의 수를 의미한다. 
            $a$는 입력이 $x$일 때 신경망의 출력 벡터이며, 합은 모든 학습 데이터 입력 $x$에 대한 값이다. 
            물론 출력 $a$는 $x, w, b$에 따라 달라지며 표기를 간단히 하기위해 이런 상관성을 나타내지 않았다. 
            $\| v \|$은 벡터 $v$의 길이를 나타낸다. 
            여기서 $C$를 이차 비용함수라 부르겠다. 
            이차 비용함수는 평균제곱오차(mean squared error, MSE)라고도 한다. 
            이차 비용함수의 형태를 살펴보면, 합의 각 항은 음수가 아니므로 $C(w,b)$은 음수가 아니다. 
            게다가 $y(x)$가 모든 학습 데이터 입력 $x$에 대한 출력 $a$에 근사할 때 비용 $C(w,b)$은 작은 값 $C(w,b) \approx 0$이 된다. 
            그래서 학습 알고리즘이 $C(w,b) \approx 0$가 되는 가중치와 편향을 찾을 수 있다면 학습 알고리즘은 좋은 성능을 낼 수 있다. 
            반대로 $C(w,b)$이 매우 클 때는 많은 수의 입력에 대한 $y(x)$의 값이 출력 $a$에 근사하지 않다는 것을 의미하며 학습 알고리즘의 성능은 떨어진다. 
            그래서 학습 알고리즘의 목표는 가중치와 편향의 함수인 비용 $C(w,b)$을 최소화하는 것이다. 
            즉, 가능한 비용을 작게 하는 가중치와 편향의 집합을 찾고자 한다. 
            이를 위해 경사 하강법이라 알려진 알고리즘을 이용할 것이다. 
          </p>

          <p>
            왜 이차 비용함수를 도입할까? 
            무엇보다 신경망이 올바르게 분류하는 이미지의 수가 중요하지 않은가? 
            왜 올바르게 분류되는 이미지 수를 직접 최대화하지 않고 이차 비용함수와 같은 다른 방법을 사용하는 걸까? 
            올바르게 분류되는 이미지의 수는 신경망의 가중치와 편향에 대한 부드러운 모양의 함수가 아닌 것이 문제이다. 
            대부분의 경우 가중치와 편향에 약간의 변화를 가할 때 올바르게 분류하는 학습 데이터 이미지의 수의 변화는 거의 없다. 
            이는 성능을 향상시키기 위해 가중치와 편향을 얼마나 바꿔야하는지 알아내기 어렵다. 
            대신 이차 비용함수같이 부드러운 비용함수를 사용하면 비용에서 성능을 향상시키기 위해 가중치와 편향을 얼마나 바꾸어야하는지 알기 쉽다. 
            이러한 이유로 이차 비용함수를 최소화하려 한다. 
            그리고 그 뒤에 분류의 정확도를 측정할 것이다.  
          </p>

          <p>
            부드러운 비용함수를 사용하더라도 왜 방정식 (6)과 같은 이차함수를 사용하는지 궁금할 것이다. 
            임의로 선택한 것은 아닐까? 
            다른 비용함수를 선택한다면 완전히 다른 가중치와 편향을 가질까? 
            이는 타당한 의심이다. 
            그리고 뒤에서 다시 한번 비용함수를 살펴보고 약간의 수정을 해볼 것이다. 
            하지만 방정식 (6)과 같은 이차 비용함수를 이용하여 신경망 학습의 기본을 잘 이해할 수 있기 때문에 당분간 이에 집중할 것이다. 
          </p>

          <p>
            간략하게 다시 말하면, 목표는 신경망을 학습시켜 이차 비용함수 $C(w, b)$를 최소화하는 가중치와 편향을 찾는 것이다. 
            이는 우량조건문제(well-posed problem)이지만 가중치 $w$와 편향 $b$의 해석, 백그라운드에 숨어있는 $\sigma$함수, 신경망 구조의 선택, MNIST 등과 같은 많은 주의 분산 구조가 있습니다. 
            대부분의 구조를 무시하고 최소화에만 집중하면 많은 부분을 이해할 수 있습니다. 
            그래서 비용함수의 구체적인 형태와 신경망의 연결 등에 대해서 당분간 신경쓰지 않도록 하겠습니다. 
            대신 다변수 함수가 주어지고 이 함수를 최소화할 것입니다. 
            최소화 문제를 풀 수 있는 경사 하강법이라 불리는 기술을 살펴보겠습니다. 
            그 뒤 신경망에 대해 최소화하려는 특정 함수로 돌아오겠습니다. 
          </p>

          <p>
            함수 $C(v)$를 최소화한다고 가정합시다. 
            이는 다변수 $v = v_1, v_2, \ldots$에 대한 실수값 함수입니다. 
            $w$와 $b$를 $v$로 치환하여 어떤 함수도 될 수 있다는 점을 강조하겠습니다. 
            더이상 신경망의 관점에서 생각하지 않습니다. 
            $C(v)$를 최소화하기 위해 $C$를 두개의 변수 $v_1$과 $v_2$에 대한 함수로 생각하면 쉽습니다. 
            <br /><br />
            <img class="contentimg" src="./images/chapter1/valley.png" alt="digits" width="500px">
            <br /><br />
            $C$가 최솟값이 되는 지점을 찾을 것입니다. 
            물론 위의 함수 그래프를 눈으로 보고 최솟값을 찾을 수 있습니다. 
            이는 제가 그래프를 단순하게 그렸기 때문입니다. 
            일반적인 함수 $C$는 많은 수의 변수를 가진 복잡한 함수일 수 있으며 눈으로 최솟값을 찾기 어려울 것입니다. 
          </p>

          <p>
            문제를 공략하는 한 방법은 미분을 사용하여 최솟값을 찾는 것입니다. 
            도함수를 구해 $C$의 극값을 찾을 수 있습니다. 
            운 좋게 함수 $C$가 하나 또는 적은 수의 변수에 대한 함수이면 이는 잘 동작할 것입니다. 
            하지만 훨씬 많은 변수에 대한 함수라면 어려울 것입니다. 
            신경망의 경우 훨씬 더 많은 변수를 다룹니다. 
            가장 큰 신경망은 복잡한 방법으로 수억개의 가중치와 편향에 의존하는 비용함수를 가집니다. 
            미분을 사용하여 이를 최적화하기에는 역부족입니다. 
          </p>

          <p>
            (
              $C$를 두 변수에 대한 함수라 생각해 통찰력을 얻은 뒤, 
              두 문단 뒤에 입장을 바꿔 "두 변수 이상일 경우 함수는 어떨까요?"라고 물을 것입니다. 
              유감스럽게 생각합니다. 
              $C$를 두 변수에 대한 함수로 생각하는 것은 정말 도움이 될 것입니다. 
              이런 이해가 무너지는 일이 발생합니다. 
              지난 두 문단에서는 이러한 붕괴에대해 다루었습니다.
              수학에 대한 좋은 생각은 다수의 직관적인 이해를 오가는 것을 포함합니다. 
              적절한 곳에 각 이해를 사용하는 것이 중요합니다. 
            )
          </p>

          <p>
            결국 미분은 동작하지 않습니다. 
            다행스럽게도 잘 동작하는 알고리즘을 제시하는 아름다운 비유가 있습니다. 
            우리의 함수를 계곡이라고 생각해봅시다. 
            그리고 공이 계곡 경사를 따라 굴러 내려간다고 상상해봅시다. 
            일상적인 경험으로 미루어볼때 공은 결국 계곡의 아래로 내려갈 것입니다. 
            이 아이디어를 이용해서 함수의 최솟값을 찾는 방법을 얻을 수 있슬 것입니다. 
            상상의 공이 시작하는 지점을 무작위로 고르고 계곡 바닥을 향해 굴러내려가는 공의 움직임을 시뮬레이션해봅시다. 
            이 시뮬레이션을 $C$의 도함수를 계산하여 간단하게 해볼 수 있습니다. 
            (아마 이차 도함수를 사용해야할 것입니다.)
            이 도함수는 계곡의 국지적 모양에 대해 알아야할 모든것을 알려줍니다. 
            따라서 공이 어떻게 움직이는지 알 수 있습니다.
          </p>

          <p>
            방금한 설명에서 공에 대해 마찰력과 중력 등을 고려햐여 뉴턴의 운동방정식을 쓸 것이라고 생각할 수 있다. 
            사실 공의 움직임을 그렇게 심오하게 살펴보지는 않을 것이다. 
            단지 $C$를 최소화하는 알고리즘을 만드는 것이지 물리 법칙을 이용해 정확한 시뮬레이션을 하려는 것은 아니다. 
            공의 관점으로 사고를 제한하지 않고 상상을 실험한다. 
            그래서 물리의 사소한 부분에 파고들지 말고 스스로에게 간단하게 물어보자. 
            하루동안 신(god)이 된다면 공이 어떻게 움직여야하는지 자신의 물리 법칙을 만들어 공이 계곡의 바닥으로 항상 굴러가게 만들수 있을까?  
          </p>

          <p>
            질문을 다시 써보자. 
            공이 $v_1$ 방향으로 $\Delta v_1$ 만큼 움직이고 $v_2$ 방향으로 $\Delta v_2$ 만큼 움직일때 어떤 일이 일어나는지 생각해보자. 
            미분을 이용해 $C$의 변화를 다음과 같이 쓸 수 있다. 
            $$\begin{eqnarray} 
            \Delta C \approx \frac{\partial C}{\partial v_1} \Delta v_1 +
            \frac{\partial C}{\partial v_2} \Delta v_2.
            \tag{7}\end{eqnarray}$$
            $\Delta C$를 음수로 만드는 $\Delta v_1$과 $\Delta v_2$를 선택하는 방법을 찾고자 한다.  
            즉 공이 계곡으로 굴러 내려가는 변수를 선택할 것이다. 
            $\Delta v$를 $v$에 관해 변화하는 벡터 $\Delta v \equiv (\Delta v_1, \Delta v_2)^T$로 정의한다. 
            여기서 $T$는 전치 행렬 연산이며 행벡터를 열벡터로 바꾼다. 
            $C$의 경사를 편미분의 벡터 $\left(\frac{\partial C}{\partial v_1}, \frac{\partial C}{\partial v_2}\right)^T$로 정의한다. 
            경사 벡터를 $\nabla C$로 표기한다. 
            $$\begin{eqnarray} 
            \nabla C \equiv \left( \frac{\partial C}{\partial v_1}, 
            \frac{\partial C}{\partial v_2} \right)^T.
            \tag{8}\end{eqnarray}$$
            잠시뒤 변화량 $\Delta C$을 $\Delta v$와 경사 $\nabla C$에 관해 쓸 것이다. 
            하지만 그 전에 경사에 대한 이해에 어려움을 껵는 사람들에 대해 몇가지를 분명히하고자 한다. 
            $\nabla C$를 처음 본 사람들은 어떻게 $\nabla$ 기호를 이해해야하는지 궁금해한다. 
            $\nabla$은 정확히 무엇을 의미할까?
            사실 $\nabla C$을 하나의 수학적 객체로 이해하여 위에서 정의한 벡터로 생각해도 좋다. 
            이는 두 가지 기호를 사용하여 표시될 수 있다. 
            이 관점에서 $\nabla$은 "$\nabla$은 경사 벡터야."라고 말하는 표기에 불과하다. 
            $\nabla$을 독립적인 수학적 개체로 보는 심화된 관점이 있지만 여기서는 불필요하다. 
          </p>

          <p>
            이 정의에 의해 $\Delta C$에 관한 식
            <span class="tooltip"><b>(7)</b>
              <span class="tooltiptext">
                $$\begin{eqnarray} 
                \Delta C \approx \frac{\partial C}{\partial v_1} \Delta v_1 +
                \frac{\partial C}{\partial v_2} \Delta v_2 \nonumber\end{eqnarray}$$
              </span>
            </span>
            은 다음과 같이 쓸 수 있다. 
            $$\begin{eqnarray} 
            \Delta C \approx \nabla C \cdot \Delta v.
            \tag{9}\end{eqnarray}$$
            $\nabla C$이 왜 경사 벡터라 불리는지 이 식을 통해 알 수 있다. 
            경사라 불리는 것이 하는 것처럼 $\nabla C$는 $C$에 관한 변화를 $v$에 관한 변화와 연관짓는다. 
            이 식에 대해 흥미로운 점은 $\Delta C$를 음수로 만드는 $\Delta v$를 어떻게 선택할지 보여준다. 
            특히 다음을 가정하자. 
            $$\begin{eqnarray} 
            \Delta v = -\eta \nabla C,
            \tag{10}\end{eqnarray}$$
            여기서 $\eta$는 학습율이라 알려진 작은 양수 매개변수이다. 
            그러면 식
            <span class="tooltip"><b>(9)</b>
              <span class="tooltiptext">
                $$\begin{eqnarray} 
                \Delta C \approx \nabla C \cdot \Delta v \nonumber\end{eqnarray}$$
              </span>
            </span>
            를 통해 
            $\Delta C \approx -\eta
            \nabla C \cdot \nabla C = -\eta \|\nabla C\|^2$
            를 알 수 있다. 
            식 <span class="tooltip"><b>(10)</b>
              <span class="tooltiptext">
                $$\begin{eqnarray} 
                \Delta v = -\eta \nabla C \nonumber\end{eqnarray}$$
              </span>
            </span>
            에 따라 $v$가 변한다면 
            $\| \nabla C \|^2 \geq 0$이므로 $\Delta C \leq 0$이되므로 $C$는 증가하지 않고 항상 감소합니다. 
            (물론 식
            <span class="tooltip"><b>(9)</b>
              <span class="tooltiptext">
                $$\begin{eqnarray} 
                \Delta C \approx \nabla C \cdot \Delta v \nonumber\end{eqnarray}$$
              </span>
            </span>
            에서 근사 경계 안에서) 
            이는 정확히 우리가 구하고자 하는 특성입니다. 
            그래서 식 
            <span class="tooltip"><b>(10)</b>
              <span class="tooltiptext">
                $$\begin{eqnarray} 
                \Delta v = -\eta \nabla C \nonumber\end{eqnarray}$$
              </span>
            </span>
            을 경사 하강 알고리즘에서 공의 "운동 법칙"을 정의한다. 
            즉, 식 
            <span class="tooltip"><b>(10)</b>
              <span class="tooltiptext">
                $$\begin{eqnarray} 
                \Delta v = -\eta \nabla C \nonumber\end{eqnarray}$$
              </span>
            </span>
            을 사용하여 $\Delta v$에 대한 값을 계산하고, 공의 위치 $v$를 그 값만큼 움직인다. 
            $$\begin{eqnarray}
            v \rightarrow v' = v -\eta \nabla C.
            \tag{11}\end{eqnarray}$$
            이 새로운 규칙을 사용해 다시 움직인다. 
            이를 계속 반복하면 $C$를 감소시켜 전역 최솟값에 도달할 수 있다.  
          </p>

          <p>
            요약하면, 경사 하강 알고리즘은 반복적으로 경사 $\nabla C$을 계산하여 계곡 경사로 떨어지는 반대방향으로 움직인다.  
            이를 아래처럼 시각화할 수 있다.
            <br /><br />
            <img class="contentimg" src="./images/chapter1/valley_with_ball.png" alt="digits" width="500px">
            <br /><br />
            이 규칙으로 경사 하강이 현실 세계의 물리 운동을 만들지는 않는다. 
            현실 세계에서 공은 가속도를 가지고 가속도는 경사를 가로질러 굴러가거나 잠시나마 언덕위로 굴러가도록 만든다. 
            공이 계곡을 따라 굴러 내려간다는 것이 보장된다는 마찰 효과 후에 일어난다. 
            그에 반해, $\Delta v$를 고르는 규칙은 "지금 바로 내려가라"고 말해준다. 
            최솟값을 찾는데 여전히 좋은 규칙이다. 
          </p>

          <p>
            경사 하강이 올바르게 동작하려면 학습률 $\eta$를 충분히 작은 값으로 골라 식
            <span class="tooltip"><b>(9)</b>
              <span class="tooltiptext">
                $$\begin{eqnarray} 
                \Delta C \approx \nabla C \cdot \Delta v \nonumber\end{eqnarray}$$
              </span>
            </span>
            가 좋은 근사를 갖도록 해야한다. 
            그렇지 않으면 $\Delta C > 0$가 되어 좋지 못한 결과를 얻는다. 
            동시에 $\eta$가 너무 작아지면 $\Delta v$의 변화가 매우 작아져 경사 하강 알고리즘의 동작은 느려진다. 
            실제 구현에서는 식 
            <span class="tooltip"><b>(9)</b>
              <span class="tooltiptext">
                $$\begin{eqnarray} 
                \Delta C \approx \nabla C \cdot \Delta v \nonumber\end{eqnarray}$$
              </span>
            </span>
            는 좋은 근사를 갖도록하는 $\eta$는 다양하지만 알고리즘은 느리지 않다. 
            어떻게 동작하는지 뒤에서 살펴보자.
          </p>

          <p>
            $C$가 두 개의 변수에 대한 함수 일때 경사 하강 알고리즘을 설명했다. 
            하지만 사실 $C$가 많은 수의 변수를 가질때에도 잘 동작한다. 
            특히 $m$개의 변수 $v_1,\ldots,v_m$에 대한 함수 $C$를 가정하자. 
            그러면 작은 변화 $\Delta v = (\Delta v_1, \ldots, \Delta v_m)^T$에 의한 $C$에서의 변화 $\Delta C$는 다음과 같다. 
            
            $$\begin{eqnarray} 
            \Delta C \approx \nabla C \cdot \Delta v,
            \tag{12}\end{eqnarray}$$

            여기서 $\nabla C$는 다음의 벡터이다. 

            $$\begin{eqnarray}
            \nabla C \equiv \left(\frac{\partial C}{\partial v_1}, \ldots, 
            \frac{\partial C}{\partial v_m}\right)^T.
            \tag{13}\end{eqnarray}$$

            두 개의 변수의 경우처럼 $\Delta v$를 고를 수 있다. 

            $$\begin{eqnarray}
            \Delta v = -\eta \nabla C,
            \tag{14}\end{eqnarray}$$

            그리고 $\Delta C$는 음수이므로 (근사)식
            <span class="tooltip"><b>(12)</b>
              <span class="tooltiptext">
                $$\begin{eqnarray} 
                \Delta C \approx \nabla C \cdot \Delta v \nonumber\end{eqnarray}$$
              </span>
            </span>
            는 보장된다. 
            이를 통해 $C$가 다변수 함수일때에도 새로운 규칙을 반복적으로 적용해 경사가 최솟값을 찾도록 할 수 있다. 

            $$\begin{eqnarray}
            v \rightarrow v' = v-\eta \nabla C.
            \tag{15}\end{eqnarray}$$

            이 새로운 규칙을 경사 하강 알고리즘을 정의로 생각할 수 있다. 
            이 새로운 규칙은 함수 $C$의 최솟값을 찾으려고 위치 $v$를 반복적으로 바꾸는 방법을 제시한다. 
            이 규칙은 항상 동작하지는 않는다. 
            몇몇 경우 잘못된 동작을 하며 경사하강은 $C$의 전역 최솟값을 찾지 못한다. 
            이는 뒤이은 챕터에서 살펴보겠다. 
            하지만 실제 경사 하강은 잘 동작하며 신경망에서 비용함수를 최소화하는 강력한 방법이다. 
            그리고 신경망의 학습을 돕는다. 
          </p>

          <p>
            경사 하강은 최솟값을 찾는 최적 전략인 면이 있다. 
            $C$를 가능한 많이 감소시키기 위해 위치에서 $\Delta v$를 움직인다고 가정하자. 
            이는 $\Delta C \approx \nabla C \cdot \Delta v$를 최소화하는 것과 같다. 
            어떤 고정된 값 $\epsilon > 0$에 대한 $\| \Delta v \| = \epsilon$를 만족하도록 위치 변화의 크기를 제한해보자. 
            즉, 고정된 크기로 위치를 변화시키고 $C$가 가능한 많이 감소하는 위치 변화 방향을 찾고자 한다. 
            $\nabla C \cdot \Delta v$를 최소화하는 $\Delta v$는 $\Delta v = - \eta \nabla C$라는 것을 증명할 수 있다. 
            여기서 $\eta = \epsilon / \|\nabla C\|$는 크기 제한 $\|\Delta v\| = \epsilon$이 결정한다. 
            그래서 경사 하강은 $C$를 가장 많이 감소하는 방향으로 내려가는 방법이라 할 수 있다.  
          </p>

          <br />
          <h2>예제</h2>

          <p>
            <ul>
              <li>
                <p>
                  이전 단락의 주장을 증명하라. 
                  힌트: 쿄시-슈바르츠 부등식을 이용하라. 
                </p>
              </li>
              
              <li>
                <p>
                  함수 $C$가 이변수 함수인 경우와 다변수 함수인 경우에 대해 경사 하강 알고리즘을 설명하였다. 
                  함수 $C$가 일변수 함수인 경우 무슨 일이 일어나는가? 
                  일차원인 경우 경사 하강 알고리즘의 동작을 기하학적으로 해석할 수 있나?
                </p>
              </li>
            </ul>
          </p>

          <p>
            사람들은 실제 물리 현상을 모방한 것을 포함한 경사 하강 알고리즘의 변형들을 많이 연구했다. 
            공의 움직임을 모방한 변형은 어떤 면에서 이점을 가지지만 주요한 단점 또한 가진다. 
            $C$의 이차 편미분을 계산해야 하며 이는 비용이 많이 든다. 
            비용이 많이 드는 이유를 살펴보자. 
            모든 이차 편도함수 $\partial^2 C/ \partial v_j \partial v_k$ 을 계산한다고 가정하자. 
            변수 $v_j$ 가 백 만개 있다면 백만의 제곱인 일억 번의 이차 편도함수 계산을 해야한다. 
            이를 계산하는 비용은 많이 든다. 
            그렇지만 이런 문제를 피하는 몇 가지 속임수가 있고, 경사 하강에 대한 대안을 찾는 것은 현재 활발하게 연구되고 있는 분야이다. 
            하지만 이 책에서 신경망 학습에 대한 방법으로 경사 하강과 그의 변형을 살펴볼 것이다. 
          </p>

          <p>
            신경망을 학습시키기 위해 경사 하강을 어떻게 적용할 수 있을까? 
            식 

            <span class="tooltip"><b>(6)</b>
              <span class="tooltiptext">
                $$\begin{eqnarray}  C(w,b) \equiv
                \frac{1}{2n} \sum_x \| y(x) - a\|^2 \nonumber\end{eqnarray}$$
              </span>
            </span>

            에서 비용을 최소화 하는 가중치 $w_k$ 와 편향 $b_l$ 을 찾기위해 경사 하강을 사용하는 것이 기본적인 생각이다. 
            어떻게 동작하는지 살펴보기 위해 경사 하강 갱신 규칙을 $v_j$를 치환하여 가중치와 편향으로 다시 써보자. 
            즉, "위치"는 $w_k$ 와 $b_l$ 을 가지며 경사 벡터 $\nabla C$ 는 상응하는 $\partial C / \partial w_k$ 와 $\partial C / \partial b_l$ 을 가진다. 
            경사 하강 갱신 규칙을 위의 요소들로 다시 쓰면 다음과 같다. 

            $$\begin{eqnarray}
            w_k & \rightarrow & w_k' = w_k-\eta \frac{\partial C}{\partial w_k} \tag{16}\\
            b_l & \rightarrow & b_l' = b_l-\eta \frac{\partial C}{\partial b_l}.
            \tag{17}\end{eqnarray}$$

            이 갱신 규칙을 반복적으로 적용하여 "언덕 아래로 굴러 내려"갈 수 있으며, 비용함수의 최솟값을 찾을 수 있다. 
            다시 말해, 이것이 바로 신경망에서 학습을 하는데 사용할 수 있는 규칙이다. 
          </p>

          <p>
            경사 하강 규칙을 적용하는데 몇 가지 어려움이 있다. 
            책의 뒷 부분에서 어려움을 깊이 살펴보도록 하자. 
            당분간은 한 가지 문제에 대해 말하고자 한다. 
            문제가 무엇인지 이해하기 위해 식

            <span class="tooltip"><b>(6)</b>
              <span class="tooltiptext">
                $$\begin{eqnarray}  C(w,b) \equiv
                \frac{1}{2n} \sum_x \| y(x) - a\|^2 \nonumber\end{eqnarray}$$
              </span>
            </span>

            에서 이차 비용함수를 다시 살펴보자. 
            이 비용함수는 $C = \frac{1}{n} \sum_x C_x$ 과 같은 형태를 가지며, 이는 각각의 학습 데이터에 대한 비용 $C_x \equiv \frac{\|y(x)-a\|^2}{2}$ 의 평균이다. 
            실제로 $\nabla C$ 을 계산하려면 각각의 학습 데이터 입력 $x$ 에 대해 개별적으로 $\nabla C_x$ 을 계산한 다음 평균해야한다. 
            그러면 $\nabla C = \frac{1}{n} \sum_x \nabla C_x$ 이 된다. 
            하지만 학습 데이터 입력 수가 많으면 계산이 오래 걸리며 학습은 매우 느리게 일어난다.  
          </p>

          <p>
            확률적 경사 하강(stochastic gradient descent)을 이용하면 학습을 빨리 할 수 있다. 
            무작위로 고른 학습 데이터 입력에 대해 $\nabla C_x$ 을 계산하여 $\nabla C$ 을 계산하는 것이 아이디어이다. 
            이 작은 표본을 평균하여 $\nabla C$ 의 빨리 측정할 수 있으며 이는 경사 하강 속도를 높이고 학습 속도 또한 빠르게 한다. 
          </p>

          <p>
            이 아이디어를 더 정확하게 써보면, 확률적 경사 하강은 무작위로 고른 학습 데이터 입력 $m$ 개를 선택하여 동작한다. 
            이렇게 무작위로 뽑힌 학습 데이터 입력을 $X_1, X_2, \ldots, X_m$ 이라 하고, 이들을 미니 배치(mini-batch)라 하자. 
            표본 크기 $m$이 충분히 크면 $\nabla C_{X_j}$ 의 평균 값은 모든 $\nabla C_x$ 값의 평균과 대략적으로 같아진다. 
            식으로 쓰면 다음과 같다. 

            $$\begin{eqnarray}
            \frac{\sum_{j=1}^m \nabla C_{X_{j}}}{m} \approx \frac{\sum_x \nabla C_x}{n} = \nabla C,
            \tag{18}\end{eqnarray}$$

            여기서 두 번째 합은 학습 데이터 전체에 대한 합이다. 
            양 변을 바꿔서 쓰면 다음과 같다. 

            $$\begin{eqnarray}
            \nabla C \approx \frac{1}{m} \sum_{j=1}^m \nabla C_{X_{j}},
            \tag{19}\end{eqnarray}$$

            무작위로 고른 미니 배치에 대한 경사를 계산하여 전체 경사를 측정할 수 있다는 점을 확인하자. 
          </p>

          <p>
            이를 신경망 학습과 연관시켜보자. 
            $w_k$ 와 $b_l$ 을 신경망의 가중치와 편향이라 가정하자. 
            그러면 확률적 경사 하강은 무작위로 선택한 학습 입력 미니 배치를 뽑아서 동작한다. 
            그리고 다음의 식으로 학습을 진행한다. 

            $$\begin{eqnarray} 
            w_k & \rightarrow & w_k' = w_k-\frac{\eta}{m}
            \sum_j \frac{\partial C_{X_j}}{\partial w_k} \tag{20}\\
            
            b_l & \rightarrow & b_l' = b_l-\frac{\eta}{m}
            \sum_j \frac{\partial C_{X_j}}{\partial b_l},
            \tag{21}\end{eqnarray}$$

            여기서 합은 현재 미니 배치에서 모든 학습 입력 $X_j$ 에 대한 합이다. 
            그런 다음 무작위로 선택한 다른 미니 배치를 골라 학습한다. 
            학습 입력 데이터가 고갈될 때 까지 학습을 진행하며 이는 학습의 한 세대(an epoch of training)를 완료했다고 말한다. 
            이 시점에서 새로운 학습 세대를 시작한다. 
          </p>

          <p>
            덧붙여 말하자면 비용 함수의 조정과 가중치와 편향을 갱신하는 미니 배치의 조정에 다양한 규칙이 있다는 점을 생각해야 한다. 
            식 

            <span class="tooltip"><b>(6)</b>
              <span class="tooltiptext">
                $$\begin{eqnarray}  C(w,b) \equiv
                \frac{1}{2n} \sum_x \| y(x) - a\|^2 \nonumber\end{eqnarray}$$
              </span>
            </span>

            에서 전체 비용 함수를 $\frac{1}{n}$ 해주었다. 
            평균이 아닌 개별 학습 데이터에 대한 합을 계산할 때 $\frac{1}{n}$ 을 빼먹는 경우가 많다. 
            전체 학습 데이터 개수가 알려지지 않은 경우에는 유용하다. 
            예를 들어 실시간으로 학습 데이터가 만들어지는 경우 이를 이용할 수 있다. 
            그리고 비슷한 방법으로 미니 배치는 규칙 

            <span class="tooltip"><b>(20)</b>
              <span class="tooltiptext">
                $$\begin{eqnarray} 
                w_k & \rightarrow & w_k' = w_k-\frac{\eta}{m}
                \sum_j \frac{\partial C_{X_j}}{\partial w_k}  \nonumber\end{eqnarray}$$
              </span>
            </span>

            과

            <span class="tooltip"><b>(21)</b>
              <span class="tooltiptext">
                $$\begin{eqnarray}  
                b_l & \rightarrow & b_l' = b_l-\frac{\eta}{m}
                \sum_j \frac{\partial C_{X_j}}{\partial b_l} \nonumber\end{eqnarray}$$
              </span>
            </span>

            을 갱신 할때 합 앞에 $\frac{1}{m}$ 을 빼먹는 경우가 있다. 
            학습률 $\eta$ 를 조정하는 것과 동등하므로 개념적으로 큰 차이는 없다. 
            하지만 다른 작업들을 상세히 비교할 때 살펴볼 가치가 있다. 
          </p>

          <p>
            확률적 경사 하강을 정치적인 여론 조사로 생각 할 수 있다. 
            전체 선거를 진행하는 것보다 여론 조사를 진행하는 것이 쉬운 것처럼 전체 배치에 대해 경사 하강을 적용하는 것보다 표본인 미니 배치에 대해 적용하는 것이 훨씬 쉽다. 
            예를 들어 MNIST처럼 학습 데이터 크기가 $n = 60,000$ 인 경우 미니 배치 크기를 $m = 10$ 으로 하면 경사를 측정하는데 $6,000$ 배 빠르게 할 수 있다. 
            물론, 통계적인 변동이 있을 수 있으므로 측정이 완벽하지는 않지만 완벽할 필요는 없다. 
            우리에게 필요한 것은 $C$ 를 감소시키는 일반적인 방향에 대한 움직임이다. 
            이는 경사의 계산을 정확하게 할 필요는 없다는 것을 의미한다. 
            실제로 확률적 경사 하강은 흔히 쓰이는 방법이고 신경망을 학습하는데 강력한 기술이다. 
            그리고 이 책에서 살펴볼 학습 방법의 기본이 된다.  
          </p>

          <br />
          <h2>예제</h2>

          <p>
            <ul>
              <li>
                <p>
                  경사 하강의 극단적인 예는 미니 배치 크기를 1로 하는 것이다. 
                  즉, 학습 데이터 입력 $x$ 가 주어지고 가중치와 편향을 $w_k \rightarrow w_k' = w_k - \eta \partial C_x / \partial w_k$ 과 $b_l \rightarrow b_l' = b_l - \eta \partial C_x / \partial b_l$ 에 따라 갱신한다. 
                  그런 다음 다른 학습 데이터를 고르고 가중치와 편향을 다시 갱신한다. 
                  이 과정을 계속 반복한다. 
                  이 절차를 온라인 학습 또는 증분식 학습(online, on-line, or incremental learning)이라 한다. 
                  온라인 학습에서 신경망은 한 번에 하나의 학습 데이터를 이용해 학습한다. 
                  미니 배치 크기가 20인 확률적 경사 하강과 비교하여 온라인 학습의 장점과 단점을 서술하여라. 
                </p>
              </li>
            </ul>
          </p>

          <p>
            경사 하강을 처음 접하는 사람들을 괴롭히는 점들을 설명하면서 이 절을 마무리하려 한다. 
            신경망에서 비용 함수 $C$는 모든 가중치와 편향을 변수로 하는 다변수 함수이고 어떤 점에서는 고차원 공간에서 면을 정의한다. 
            몇몇 사람들은 다음과 같은 생각에 마주한다. 
            "이 차원을 시각화할 수 있어야한다."
            그리고 다음과 같은 걱정을 한다. 
            "5차원은 커녕 사차원도 상상할 수 없다."
            진정한 수학자가 가지는 어떤 특별한 능력이 있을까? 
            물론 없다. 
            전문적인 수학자라 할지라도 사차원을 시각화할 수 없다. 
            대신 수학자들이 무슨 일이 일어나는지 표시하기 위해 다른 방법을 사용한다. 
            그것은 바로 우리가 위에서 살표본 것들이다. 
            $C$ 를 감소시키기 위해 어떻게 움직이는지 이해하려고 $\Delta C$ 를 표현하는 시각적 표현 대신에 수식을 사용했다. 
            고차원을 생각하는데 뛰어난 사람들은 이러한 것을 따라 다양한 기술들을 가지고 있다. 
            수학적 표현은 하나의 예이다. 
            이러한 기술은 삼차원을 시각화하는 것 만큼 단순함을 가지지 못하지만 이러한 기술을 한 번 습득하면 고차원을 아주 잘 생각할 수 있다. 
            여기서는 더 깊게 들어가지 않겠다. 
            하지만 수학자가 고차원을 생각하는 방법에 대한 <a href="https://mathoverflow.net/questions/25983/intuitive-crutches-for-higher-dimensional-thinking">다음 글</a>을 읽어보면 좋다. 
            몇몇 기술은 꽤 복잡하지만 대부분의 내용은 직간적이며 누구나 공부할 수 있다.  
          </p>

          <br />
          <h2>숫자를 분류하는 신경망 구현하기</h2>
          <hr>

          <p>
            확률적 경사 하강과 MNIST 학습 데이터를 이용해서 손글씨 숫자를 인식하는 방법을 학습하는 프로그램을 작성해보자. 
            파이썬 (2.7) 프로그램을 이용해 단 74줄의 코드로 작성하겠다. 
            먼저 필요한 것은 MNIST 데이터이다. 
            git 사용자라면 이 책의 코드 저장소를 클론(cloning)해서 데이터를 얻을 수 있다. 
            git 사용자가 아니라면 <a href="https://github.com/mnielsen/neural-networks-and-deep-learning/archive/master.zip">여기서</a> 데이터와 코드를 다운받을 수 있다. 
          </p>

          <p>
            MNIST 데이터를 처음에 설명할 때 $60,000$ 개의 학습 이미지와 $10,000$ 개의 테스트 이미지로 구성된다고 했다. 
            이것이 공식적인 MNIST이다. 
            사실 여기서는 데이터를 조금 다른 방식으로 나눌 것이다. 
            테스트 이미지는 그대로 남겨두고 $60,000$ 개의 학습 데이터를 두 부분으로 나누겠다. 
            $50,000$ 개의 이미지를 이용해 신경망을 학습시키고, 다른 $10,000$ 개의 이미지를 검증 데이터셋(validation set)으로 사용하겠다. 
            이 장에서는 검증 데이터를 사용하지 않을거다. 
            하지만 책의 뒷 부분에서 학습률과 같은 신경망의 하이퍼파라미터(hyper-parameters)를 설정하는 방법을 찾는데 검증 데이터셋을 이용할 것이다. 
            하이퍼파라미터는 학습 알고리즘이 직접 선택하지 않는다. 
            검증 데이터는 원래의 MNIST 데이터의 한 부분이 아니지만 대부분의 사람들은 이러한 방식으로 MNIST를 이용하고 검증 데이터의 사용은 신경망에서 흔한 방법이다. 
            앞으로 "MNIST 학습 데이터"는 $60,000$ 개의 원본 이미지 데이터셋
            <span class="tooltip"><b>*</b>
              <span class="tooltiptext">
                As noted earlier, the MNIST data set is based on two data sets collected by NIST, <br />
                the United States' National Institute of Standards and Technology. <br />
                To construct MNIST the NIST data sets were stripped down and <br />
                put into a more convenient format by Yann LeCun, Corinna Cortes, <br />
                and Christopher J. C. Burges. See this link for more details. <br />
                The data set in my repository is in a form that makes it easy <br />
                to load and manipulate the MNIST data in Python. <br />
                I obtained this particular form of the data <br /> 
                from the LISA machine learning laboratory <br /> 
                at the University of Montreal (link). 
              </span>
            </span>
            이 아닌 $50,000$ 개의 이미지 데이터 셋을 의미한다.  
          </p>

          <p>
            MNIST 데이터외에 선형대수 계산을 빨리 하기위해 <a href="http://www.numpy.org/">Numpy</a>라 불리는 파이썬 라이브러리가 필요하다. 
            Numpy 설치는 <a href="https://www.scipy.org/install.html">여기서</a> 할 수 있다. 
          </p>

          <p>
            전체 코드를 보여주기에 앞서 신경망 코드의 핵심적인 특징을 설명하려 한다. 
            주목할만한 부분은 <code>Network</code> 클래스이다.
            이를 이용해 신경망을 표현한다. 
            아래 코드를 이용해 <code>Network</code> 객체를 초기화한다. 

            <pre class="prettyprint linenums:1"><code class="language-python"></code>class Network(object):
    def __init__(self, sizes):
        self.num_layers = len(sizes)
        self.sizes = sizes
        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]
        self.weights = [np.random.randn(y, x) 
            for x, y in zip(sizes[:-1], sizes[1:])]</code></pre>

            이 코드에서 리스트 <code>sizes</code>는 각 층의 뉴런 수를 나타낸다. 
            예를 들어, 첫 번째 층에 2개의 뉴런, 두 번째 층에 3개의 뉴런, 마지막 층에 1개의 뉴런을 가진 <code>Network</code> 객체를 만드려면 아래와 같은 코드를 작성하면 된다. 

            <pre class="prettyprint linenums:1"><code class="language-python"></code>    net = Network([2, 3, 1])</code></pre>

            <code>Network</code> 객체에서 편향과 가중치는 모두 무작위로 초기화된다. 
            초기화를 할때 Numpy <code>np.random.randn</code> 함수를 하영하여 평균은 $0$ 이고 표준 편차가 $1$ 인 가우스 분포를 생성한다. 
            이 무작위 초기화는 확률적 경사 하강 알고리즘의 시작점이 된다. 
            이 장의 뒷부분에서 가중치와 편향을 초기화하는 더 좋은 방법을 살펴볼 것이다. 
            하지만 당분간은 이렇게 한다. 
            <code>Network</code> 초기화 코드는 첫 번째 뉴런층은 입력층이라 가정하고 이 뉴런에 대한 편향 값 설정은 하지 않는다. 
            편향은 이 후의 층에서 출력을 계산할 때만 사용되기 때문이다. 
          </p>

          <p>
            Numpy 행렬의 리스트에 가중치와 편향이 저장된다. 
            예를 들어 <code>net.weights[1]</code>은 두 번째와 세 번째 뉴런 층을 연결하는 가중치를 저장하는 Numpy 행렬이다. 
            (파이썬 리스트의 인덱스는 <code>0</code>부터 시작하므로 이는 첫 번째와 두 번째 층 연결이 아니다.) 
            <code>net.weights[1]</code>을 단순하게 행렬 $w$ 로 표기하자. 
            $w_jk$ 는 두 번째 층의 $k^{\rm th}$ 번째 뉴런과 세 번째 층의 $j^{\rm th}$ 번째 뉴런을 연결하는 가중치이다. 
            인덱스 $j$ 와 $k$ 의 순서는 다소 어색할 수 있다. 
            $j$ 와 $k$ 의 순서를 바꾸는 것이 더 말이 될 수도 있다.
            이러한 순서를 사용하는 이점은 세 번째 층의 활성 벡터는 다음과 같다는 의미이다. 
            
            $$\begin{eqnarray} 
            a' = \sigma(w a + b).
            \tag{22}\end{eqnarray}$$

            위 식은 많은 것을 포함하므로 조금 더 풀어써보자. 
            $a$ 는 두 번째 층의 뉴런을 활성화하는 벡터이다. 
            $a'$ 을 얻기 위해 $a$ 에 가중치 행렬 $w$ 를 곱하고 편향 벡터 $b$ 를 더한다. 
            그런 다음 벡터 $w a +b$ 의 모든 원소에 함수 $\sigma$ 를 적용한다. 
            (이는 함수 $\sigma$ 를 벡터화(vectorizing)한다고 불린다.) 
            시그모이드 뉴런의 출력을 계산할 때 식 
            <span class="tooltip"><b>(22)</b>
              <span class="tooltiptext">
                $$\begin{eqnarray} 
                a' = \sigma(w a + b) \nonumber\end{eqnarray}$$
              </span>
            </span>
            는 이전의 규칙 식
            <span class="tooltip"><b>(4)</b>
              <span class="tooltiptext">
                $$\begin{eqnarray} 
                \frac{1}{1+\exp(-\sum_j w_j x_j-b)} \nonumber\end{eqnarray}$$
              </span>
            </span>
            와 같은 결과를 준다는 사실을 쉽게 증명할 수 있다. 
          </p>

          <br />
          <h2>예제</h2>

          <p>
            <ul>
              <li>
                <p>
                  식
                  <span class="tooltip"><b>(22)</b>
                    <span class="tooltiptext">
                      $$\begin{eqnarray} 
                      a' = \sigma(w a + b) \nonumber\end{eqnarray}$$
                    </span>
                  </span>
                  를 원소 형태로 쓰고 시그모이드 뉴런의 출력을 계산하는 규칙 
                  <span class="tooltip"><b>(4)</b>
                    <span class="tooltiptext">
                      $$\begin{eqnarray} 
                      \frac{1}{1+\exp(-\sum_j w_j x_j-b)} \nonumber\end{eqnarray}$$
                    </span>
                  </span>
                  와 같은 결과를 주는 것을 증명해라. 
                </p>
              </li>
            </ul>
          </p>

          <p>
            이 모든 것을 고려해서 <code>Network</code> 인스턴스(instance)에서 출력을 계산하는 코드를 작성하는 것은 쉽다.
            시그모이드 함수 정의로 시작하자. 
            <pre class="prettyprint linenums:1"><code class="language-python"></code>    def sigmoid(z):
        return 1.0/(1.0+np.exp(-z))</code></pre>
            입력 $z$ 가 벡터이거나 Numpy array일 때 Numpy는 자동으로 <code>sigmoid</code> 함수를 원소별로 적용해서 벡터 형태로 출력한다. 
          </p>

          <p>
            그 다음 <code>Network</code> 클래스에 <code>feedforward</code> 메소드를 추가한다. 
            신경망에 대한 입력이 주어지면 상응하는 출력
            <span class="tooltip"><b>*</b>
              <span class="tooltiptext">
                블라블라블라
              </span>
            </span>
            을 반환하는 메소드이다. 
            메소드가 하는 일은 각 층에 식 
            <span class="tooltip"><b>(22)</b>
              <span class="tooltiptext">
                $$\begin{eqnarray} 
                a' = \sigma(w a + b) \nonumber\end{eqnarray}$$
              </span>
            </span>
            를 적용하는 것이다. 

            <pre class="prettyprint linenums"><code class="language-python">    def feedforward(self, a):
    """Return the output of the network if "a" is input."""
      for b, w in zip(self.biases, self.weights):
          a = sigmoid(np.dot(w, a)+b)
          return a</code></pre>

            물론 <code>Network</code> 객체가 주로 하는 일은 학습을 하는 것이다. 
            그러기 위해서 확률적 경사 하강을 구현하는 <code>SGD</code> 메소드를 작성하자. 
            아래에 코드가 있다. 
            이해가 어려운 부분이 몇 군데 있지만 아래 부분에서 세세하게 다루겠다. 


            <pre class="prettyprint linenums"><code class="language-python">    def SGD(self, training_data, epochs, mini_batch_size, eta,
            test_data=None):
        """Train the neural network using mini-batch stochastic
        gradient descent.  The "training_data" is a list of tuples
        "(x, y)" representing the training inputs and the desired
        outputs.  The other non-optional parameters are
        self-explanatory.  If "test_data" is provided then the
        network will be evaluated against the test data after each
        epoch, and partial progress printed out.  This is useful for
        tracking progress, but slows things down substantially."""
        if test_data: n_test = len(test_data)
        n = len(training_data)
        for j in xrange(epochs):
            random.shuffle(training_data)
            mini_batches = [
                training_data[k:k+mini_batch_size]
                for k in xrange(0, n, mini_batch_size)]
            for mini_batch in mini_batches:
                self.update_mini_batch(mini_batch, eta)
            if test_data:
                print "Epoch {0}: {1} / {2}".format(
                    j, self.evaluate(test_data), n_test)
            else:
                print "Epoch {0} complete".format(j)</code></pre>
            
            <code>training_data</code>는 학습 데이터 입력과 상응하는 출력을 나타내는 튜플 <code>(x, y)</code>의 리스트이다. 
            변수 <code>epochs</code>와 <code>mini_batch_size</code>는 학습을 진행하는 세대의 수와 표본을 추출할 때 사용하는 미니 배치의 크기를 의미한다. 
            <code>eta</code>는 학습률 $\eta$이다. 
            옵션 인자 <code>test_data</code>가 주어지면 프로그램은 학습 세대 마다 신경망을 평가하고, 부분적인 진행 상황을 출력한다. 
            이는 진행 상황을 보는데 유용하지만 성능은 상당히 느려진다. 
          </p>

          <p>
            코드는 다음처럼 동작한다. 
            각 세대마다 학습 데이터를 무작위로 섞으면서 시작한다. 
            그리고 적절한 크기의 미니 배치로 나눈다. 
            이는 학습 데이터에서 무작위로 표본을 추출하는 쉬운 방법이다. 
            코드 <code>self.update_mini_batch(mini_batch, eta)</code> 부분이 이를 실행하며 <code>mini_batch</code>에서 학습 데이터를 사용하여 경사 하강의 반복에 따라 신경망의 가중치와 편향을 갱신한다. 
            <code>update_mini_batch</code> 메소드에 대한 코드는 아래와 같다. 

            <pre class="prettyprint linenums"><code class="language-python">    def update_mini_batch(self, mini_batch, eta):
        """Update the network's weights and biases by applying
        gradient descent using backpropagation to a single mini batch.
        The "mini_batch" is a list of tuples "(x, y)", and "eta"
        is the learning rate."""
        nabla_b = [np.zeros(b.shape) for b in self.biases]
        nabla_w = [np.zeros(w.shape) for w in self.weights]
        for x, y in mini_batch:
            delta_nabla_b, delta_nabla_w = self.backprop(x, y)
            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]
            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]
        self.weights = [w-(eta/len(mini_batch))*nw 
                        for w, nw in zip(self.weights, nabla_w)]
        self.biases = [b-(eta/len(mini_batch))*nb 
                        for b, nb in zip(self.biases, nabla_b)]</code></pre>

            아래의 코드가 대부분의 작업을 수행한다. 
            <pre class="prettyprint linenums"><code class="language-python">            delta_nabla_b, delta_nabla_w = self.backprop(x, y)</code></pre>

            이 코드는 역전파 알고리즘(backpropagation algorithm)을 호출한다. 
            이 알고리즘은 비용 함수의 경사를 빠르게 계산한다. 
            <code>update_mini_batch</code>는 <code>mini_batch</code>에서 모든 학습 데이터에 대한 경사를 계산한다. 그리고 <code>self.weights</code>와 <code>self.biases</code>를 갱신한다. 
          </p>

          <p>
            <code>self.backprop</code>에 대한 코드를 지금 당장 보여주지는 않을 것이다. 
            다음 장에서 역전파 알고리즘이 어떻게 동작하는지 공부하고 <code>self.backprop</code>에 대한 코드를 추가해보자. 
            당분간은 위에서 말한대로 역전파 알고리즘이 동작한다고 가정하고 학습 데이터 $x$와 연관된 비용에 대한 경사를 반환한다.
          </p>

          <p>
            위에서 생략한 문서화 문자열을 포함한 전체 프로그램을 보자. 
            <code>self.backprop</code>을 제외하고 프로그램에 대해 설명해보자. 
            앞서 말한 <code>self.SGD</code>와 <code>self.update_mini_batch</code>가 대부분의 일을 한다. 
            
          </p>


          <p>
            Temp
          </p>

          <p>뉴럴 네트워크와 딥러닝은 현재 이미지 인식과 음성 인식, 자연어 처리 분야에서 많은 문제를 푸는 최고의 해결책을 제공합니다. 이 책에서 뉴럴 네트워크와 딥러닝 뒤에 숨겨진 핵심 개념들을 다룹니다.</p>
          <span class="tooltip">툴팁<span class="tooltiptext">텍스트</span></span>
          <span class="tooltip">*<span class="tooltiptext">텍스트</span></span>
          <p>
            $$\begin{eqnarray}
              \mbox{출력} & = & \left\{ \begin{array}{ll}
                  0 & \mbox{if } \sum_j w_j x_j \leq \mbox{ threshold} \\
                  1 & \mbox{if } \sum_j w_j x_j > \mbox{ threshold}
                  \end{array} \right.
            \tag{1}\end{eqnarray}$$
          </p>
        </div>
      </div>

      <div class="rightcolumn">
        
        <div class="card">
          <h2>저자</h2>
          <h3><a href="http://michaelnielsen.org/">Michael A. Nielsen</a></h3>
        </div>

        <div class="card">
            <h2>역자</h2>
            <h3>김시현</h3>
            <p><a href="https://github.com/sihyeon-kim"><img src="images/GitHub-Mark-32px.png" alt="Github" width="17" height="17">&nbsp;sihyeon-kim</a></p>
            <p><a href="mailto:sihyeonkim0923@gmail.com">&nbsp;sihyeonkim0923@gmail.com</a></p>
        </div>

        <div class="card">
            <h2>참고 자료</h2>
            <p><a href="https://twitter.com/michael_nielsen">저자(Michael A. Nielsen) 트위터</a></p>
            <p><a href="faq.html">자주 묻는 질문</a></p>
            <p>
              <a href="https://github.com/mnielsen/neural-networks-and-deep-learning">
              저자(Michael A. Nielsen) 코드 저장소</a>
            </p>
            <p><a href="http://www.deeplearningbook.org/">Deep Learning</a>, book by Ian Goodfellow, Yoshua Bengio, and Aaron Courville</p>
            <p><a href="http://cognitivemedium.com/">cognitivemedium.com</a></p>
            <p>한글 글꼴(korean fonts): <a href="https://help.naver.com/support/contents/contents.help?serviceNo=1074&categoryNo=3497">나눔 글꼴</a></p>
        </div>
      </div>
    </div>
    
    <div class="footer">

      <p> 
        In academic work,
        please cite this book as: Michael A. Nielsen, "Neural Networks and
        Deep Learning", Determination Press, 2015
        
        <br/>
        <br/>
        
        This work is licensed under a 
        <a rel="license"  href="http://creativecommons.org/licenses/by-nc/3.0/deed.en_GB">
          Creative Commons Attribution-NonCommercial 3.0 Unported License
        </a>.  This means you're free to copy, share, and
        build on this book, but not to sell it.  If you're interested in
        commercial use, please <a href="mailto:mn@michaelnielsen.org">contact me(Michael A. Nielsen)</a>.
      </p>

      <br/>

      <p>
        학업적으로 이용 시 다음과 같이 인용해 주세요: Michael A. Nielsen, "Neural Networks and
        Deep Learning", Determination Press, 2015
        <br/><br/>
        <a rel="license" href="http://creativecommons.org/licenses/by-nc/3.0/">
          <img alt="크리에이티브 커먼즈 라이선스" style="border-width:0" src="https://i.creativecommons.org/l/by-nc/3.0/88x31.png" />
        </a>
        <br />이 저작물은 
        <a rel="license" href="http://creativecommons.org/licenses/by-nc/3.0/">크리에이티브 커먼즈 저작자표시-비영리 3.0 Unported 라이선스</a>
        에 따라 이용할 수 있습니다.
        <br /><br />
        상업적 이용을 원하면, <a href="mailto:mn@michaelnielsen.org">저자(Michael A. Nielsen)에게 연락을 주세요.</a>
        <br /><br />
        마지막 깁고 더함: 19/06/21
      </p>
    </div>

  </body>

</html>